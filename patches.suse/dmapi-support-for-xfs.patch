From: Ben Myers <bpm@sgi.com>
Date: Tue, 10 Dec 2013 20:24:43 -0600
Subject: DMAPI support for xfs
References: fate#320558
Patch-mainline: Never (dmapi was refused upstream)

This is dmapi support for xfs ported to SLE12, and rolled up into a single
patch for bisect-ability.

[update for SLE12-SP2: jeffm]
- Extensively updated from SLE12-SP1 implementation.
  * Using negative errnos everywhere
  * Uses the read_iter/write_iter interface
  * uses current->journal_info to track DMAPI status
    - XFS_ATTR_DMI was removed, as were the flags fields for several consumers
  * Cleaned up (badly) open coded versions of user_path
  * Killed all uses of strnlen_user

Signed-off-by: Ben Myers <bpm@sgi.com>
Acked-by: Jeff Mahoney <jeffm@suse.com>

---
 fs/xfs/Kconfig           |   13 
 fs/xfs/Makefile          |    3 
 fs/xfs/dmapi/Makefile    |   28 
 fs/xfs/dmapi/xfs_dm.c    | 3166 +++++++++++++++++++++++++++++++++++++++++++++++
 fs/xfs/dmapi/xfs_dm.h    |   38 
 fs/xfs/libxfs/xfs_bmap.c |    1 
 fs/xfs/xfs_bmap_util.c   |   95 +
 fs/xfs/xfs_bmap_util.h   |    3 
 fs/xfs/xfs_dmapi.h       |  234 +++
 fs/xfs/xfs_dmapi_priv.h  |   28 
 fs/xfs/xfs_dmops.c       |   72 +
 fs/xfs/xfs_file.c        |  181 ++
 fs/xfs/xfs_inode.c       |  103 +
 fs/xfs/xfs_inode.h       |    1 
 fs/xfs/xfs_ioctl.c       |   34 
 fs/xfs/xfs_ioctl32.c     |    7 
 fs/xfs/xfs_iops.c        |   41 
 fs/xfs/xfs_itable.c      |   12 
 fs/xfs/xfs_itable.h      |    8 
 fs/xfs/xfs_ksyms.c       |   91 +
 fs/xfs/xfs_linux.h       |    4 
 fs/xfs/xfs_mount.h       |   10 
 fs/xfs/xfs_qm.c          |    3 
 fs/xfs/xfs_super.c       |   71 -
 fs/xfs/xfs_super.h       |    7 
 fs/xfs/xfs_symlink.c     |   32 
 fs/xfs/xfs_trace.c       |    5 
 fs/xfs/xfs_trace.h       |    1 
 28 files changed, 4239 insertions(+), 53 deletions(-)
 create mode 100644 fs/xfs/dmapi/Makefile
 create mode 100644 fs/xfs/dmapi/xfs_dm.c
 create mode 100644 fs/xfs/dmapi/xfs_dm.h
 create mode 100644 fs/xfs/xfs_dmapi.h
 create mode 100644 fs/xfs/xfs_dmapi_priv.h
 create mode 100644 fs/xfs/xfs_dmops.c
 create mode 100644 fs/xfs/xfs_ksyms.c

--- a/fs/xfs/Kconfig
+++ b/fs/xfs/Kconfig
@@ -39,6 +39,19 @@ config XFS_QUOTA
 	  with or without the generic quota support enabled (CONFIG_QUOTA) -
 	  they are completely independent subsystems.
 
+config XFS_DMAPI
+	tristate "XFS DMAPI support"
+	depends on XFS_FS
+	select DMAPI
+	help
+	  The Data Management API is a system interface used to implement
+	  the interface defined in the X/Open document:
+	  "Systems Management: Data Storage Management (XDSM) API",
+	  dated February 1997.  This interface is used by hierarchical
+	  storage management systems.
+
+	  If unsure, say N.
+
 config XFS_POSIX_ACL
 	bool "XFS POSIX ACL support"
 	depends on XFS_FS
--- a/fs/xfs/Makefile
+++ b/fs/xfs/Makefile
@@ -68,6 +68,7 @@ xfs-y				+= xfs_aops.o \
 				   xfs_buf.o \
 				   xfs_dir2_readdir.o \
 				   xfs_discard.o \
+				   xfs_dmops.o \
 				   xfs_error.o \
 				   xfs_export.o \
 				   xfs_extent_busy.o \
@@ -81,6 +82,7 @@ xfs-y				+= xfs_aops.o \
 				   xfs_iops.o \
 				   xfs_inode.o \
 				   xfs_itable.o \
+				   xfs_ksyms.o \
 				   xfs_message.o \
 				   xfs_mount.o \
 				   xfs_mru_cache.o \
@@ -114,6 +116,7 @@ xfs-$(CONFIG_XFS_QUOTA)		+= xfs_dquot.o
 				   xfs_qm_bhv.o \
 				   xfs_qm.o \
 				   xfs_quotaops.o
+obj-$(CONFIG_XFS_DMAPI)		+= dmapi/
 
 # xfs_rtbitmap is shared with libxfs
 xfs-$(CONFIG_XFS_RT)		+= xfs_rtalloc.o
--- /dev/null
+++ b/fs/xfs/dmapi/Makefile
@@ -0,0 +1,28 @@
+#
+# Copyright (c) 2006 Silicon Graphics, Inc.
+# All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU General Public License as
+# published by the Free Software Foundation.
+#
+# This program is distributed in the hope that it would be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write the Free Software Foundation,
+# Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+#
+
+EXTRA_CFLAGS += -I$(src)/.. -I$(src)/../libxfs
+EXTRA_CFLAGS += -I$(srctree)/fs/dmapi
+
+ifeq ($(CONFIG_XFS_DEBUG),y)
+	EXTRA_CFLAGS += -g -DDEBUG
+endif
+
+obj-$(CONFIG_XFS_DMAPI)		+= xfs_dmapi.o
+
+xfs_dmapi-y			+= xfs_dm.o
--- /dev/null
+++ b/fs/xfs/dmapi/xfs_dm.c
@@ -0,0 +1,3166 @@
+/*
+ * Copyright (c) 2000-2006,2011 SGI.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#include "xfs.h"
+#include "xfs_fs.h"
+#include "xfs_types.h"
+#include "xfs_bit.h"
+#include "xfs_log_format.h"
+#include "xfs_log.h"
+#include "xfs_format.h"
+#include "xfs_trans.h"
+#include "xfs_trans_resv.h"
+#include "xfs_sb.h"
+#include "xfs_alloc.h"
+#include "xfs_dmapi.h"
+#include "xfs_mount.h"
+#include "xfs_da_format.h"
+#include "xfs_da_btree.h"
+#include "xfs_dir2.h"
+#include "xfs_dir2_priv.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_attr_sf.h"
+#include "xfs_inode.h"
+#include "xfs_icache.h"
+#include "xfs_btree.h"
+#include "xfs_ialloc.h"
+#include "xfs_itable.h"
+#include "xfs_bmap.h"
+#include "xfs_acl.h"
+#include "xfs_attr.h"
+#include "xfs_attr_leaf.h"
+#include "xfs_inode_item.h"
+#include "xfs_bmap_util.h"
+#include <dmapi.h>
+#include <dmapi_kern.h>
+#include "xfs_shared.h"
+#include "xfs_pnfs.h"
+#include "xfs_trace.h"
+#include "xfs_dm.h"
+
+#include <linux/mount.h>
+#include <linux/namei.h>
+
+#define MAXNAMLEN MAXNAMELEN
+
+#define MIN_DIO_SIZE(mp)		((mp)->m_sb.sb_sectsize)
+#define MAX_DIO_SIZE(mp)		(INT_MAX & ~(MIN_DIO_SIZE(mp) - 1))
+
+static void up_rw_sems(struct inode *ip, int flags)
+{
+	if (flags & DM_FLAGS_IMUX)
+		mutex_unlock(&ip->i_mutex);
+}
+
+static void down_rw_sems(struct inode *ip, int flags)
+{
+	if (flags & DM_FLAGS_IMUX)
+		mutex_lock(&ip->i_mutex);
+	if (flags & DM_FLAGS_IALLOCSEM_WR)
+		inode_dio_wait(ip);
+}
+
+
+/* Structure used to hold the on-disk version of a dm_attrname_t.  All
+   on-disk attribute names start with the 8-byte string "SGI_DMI_".
+*/
+
+typedef struct	{
+	char	dan_chars[DMATTR_PREFIXLEN + DM_ATTR_NAME_SIZE + 1];
+} dm_dkattrname_t;
+
+/* Structure used by xfs_dm_get_bulkall(), used as the "private_data"
+ * that we want xfs_bulkstat to send to our formatter.
+ */
+typedef struct {
+	dm_fsid_t	fsid;
+	void __user	*laststruct;
+	dm_dkattrname_t	attrname;
+} dm_bulkstat_one_t;
+
+/* In the on-disk inode, DMAPI attribute names consist of the user-provided
+   name with the DMATTR_PREFIXSTRING pre-pended.  This string must NEVER be
+   changed!
+*/
+
+static	const	char	dmattr_prefix[DMATTR_PREFIXLEN + 1] = DMATTR_PREFIXSTRING;
+
+static	dm_size_t  dm_min_dio_xfer = 0; /* direct I/O disabled for now */
+
+
+/* See xfs_dm_get_dmattr() for a description of why this is needed. */
+
+#define XFS_BUG_KLUDGE	256	/* max size of an in-inode attribute value */
+
+#define DM_MAX_ATTR_BYTES_ON_DESTROY	256
+
+#define DM_STAT_SIZE(dmtype,namelen)	\
+	(sizeof(dmtype) + sizeof(dm_handle_t) + namelen)
+
+#define DM_STAT_ALIGN		(sizeof(__uint64_t))
+
+/* DMAPI's E2BIG == EA's ERANGE */
+#define DM_EA_XLATE_ERR(err) { if (err == -ERANGE) err = -E2BIG; }
+
+static inline size_t dm_stat_align(size_t size)
+{
+	return (size + (DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+}
+
+static inline size_t dm_stat_size(size_t namelen)
+{
+	return dm_stat_align(sizeof(dm_stat_t) + sizeof(dm_handle_t) + namelen);
+}
+
+/*
+ *	xfs_dm_send_data_event()
+ *
+ *	Send data event to DMAPI.  Drop IO lock (if specified) before
+ *	the dm_send_data_event() call and reacquire it afterwards.
+ */
+int
+xfs_dm_send_data_event(
+	dm_eventtype_t	event,
+	xfs_inode_t	*ip,
+	xfs_off_t	offset,
+	size_t		length,
+	int		flags,
+	int		*lock_flags)
+{
+	struct inode	*inode = VFS_I(ip);
+	int		error;
+	uint16_t	dmstate;
+
+	do {
+		dmstate = ip->i_d.di_dmstate;
+		if (lock_flags)
+			xfs_iunlock(ip, *lock_flags);
+
+		up_rw_sems(inode, flags);
+
+		error = dm_send_data_event(event, inode, DM_RIGHT_NULL,
+				offset, length, flags);
+
+		down_rw_sems(inode, flags);
+
+		if (lock_flags)
+			xfs_ilock(ip, *lock_flags);
+	} while (!error && (ip->i_d.di_dmstate != dmstate));
+
+	return error;
+}
+
+/*	prohibited_mr_events
+ *
+ *	Return event bits representing any events which cannot have managed
+ *	region events set due to memory mapping of the file.  If the maximum
+ *	protection allowed in any pregion includes PROT_WRITE, and the region
+ *	is shared and not text, then neither READ nor WRITE events can be set.
+ *	Otherwise if the file is memory mapped, no READ event can be set.
+ *
+ */
+STATIC int
+prohibited_mr_events(
+	struct address_space *mapping)
+{
+	int prohibited = (1 << DM_EVENT_READ);
+
+	if (!mapping_mapped(mapping))
+		return 0;
+
+	i_mmap_lock_read(mapping);
+	if (mapping_writably_mapped(mapping))
+		prohibited |= (1 << DM_EVENT_WRITE);
+	i_mmap_unlock_read(mapping);
+
+	return prohibited;
+}
+
+#ifdef	DEBUG_RIGHTS
+STATIC int
+xfs_vp_to_hexhandle(
+	struct inode	*inode,
+	u_int		type,
+	char		*buffer)
+{
+	dm_handle_t	handle;
+	u_char		*ip;
+	int		length;
+	int		error;
+	int		i;
+
+	/*
+	 * XXX: dm_vp_to_handle doesn't exist.
+	 * 	Looks like this debug code is rather dead.
+	 */
+	if ((error = dm_vp_to_handle(inode, &handle)))
+		return error;
+
+	if (type == DM_FSYS_OBJ) {	/* a filesystem handle */
+		length = DM_FSHSIZE;
+	} else {
+		length = DM_HSIZE(handle);
+	}
+	for (ip = (u_char *)&handle, i = 0; i < length; i++) {
+		*buffer++ = "0123456789abcdef"[ip[i] >> 4];
+		*buffer++ = "0123456789abcdef"[ip[i] & 0xf];
+	}
+	*buffer = '\0';
+	return 0;
+}
+#endif	/* DEBUG_RIGHTS */
+
+
+
+
+/* Copy in and validate an attribute name from user space.  It should be a
+   string of at least one and at most DM_ATTR_NAME_SIZE characters.  Because
+   the dm_attrname_t structure doesn't provide room for the trailing NULL
+   byte, we just copy in one extra character and then zero it if it
+   happens to be non-NULL.
+*/
+
+STATIC int
+xfs_copyin_attrname(
+	dm_attrname_t	__user *from,	/* dm_attrname_t in user space */
+	dm_dkattrname_t *to)		/* name buffer in kernel space */
+{
+	long len;
+
+	strcpy(to->dan_chars, dmattr_prefix);
+	len = strncpy_from_user(to->dan_chars + DMATTR_PREFIXLEN,
+				from->an_chars, DM_ATTR_NAME_SIZE);
+	to->dan_chars[sizeof(to->dan_chars) - 1] = '\0';
+
+	if (len < 0)
+		return len;
+	else if (to->dan_chars[DMATTR_PREFIXLEN] == '\0')
+		return -EINVAL;
+	return 0;
+}
+
+
+/*
+ * Convert the XFS flags into their DMAPI flag equivalent for export
+ */
+STATIC uint
+_xfs_dic2dmflags(
+	__uint16_t		di_flags)
+{
+	uint			flags = 0;
+
+	if (di_flags & XFS_DIFLAG_ANY) {
+		if (di_flags & XFS_DIFLAG_REALTIME)
+			flags |= DM_XFLAG_REALTIME;
+		if (di_flags & XFS_DIFLAG_PREALLOC)
+			flags |= DM_XFLAG_PREALLOC;
+		if (di_flags & XFS_DIFLAG_IMMUTABLE)
+			flags |= DM_XFLAG_IMMUTABLE;
+		if (di_flags & XFS_DIFLAG_APPEND)
+			flags |= DM_XFLAG_APPEND;
+		if (di_flags & XFS_DIFLAG_SYNC)
+			flags |= DM_XFLAG_SYNC;
+		if (di_flags & XFS_DIFLAG_NOATIME)
+			flags |= DM_XFLAG_NOATIME;
+		if (di_flags & XFS_DIFLAG_NODUMP)
+			flags |= DM_XFLAG_NODUMP;
+	}
+	return flags;
+}
+
+STATIC uint
+xfs_ip2dmflags(
+	xfs_inode_t	*ip)
+{
+	return _xfs_dic2dmflags(ip->i_d.di_flags) |
+			(XFS_IFORK_Q(ip) ? DM_XFLAG_HASATTR : 0);
+}
+
+/*
+ * Pull out both ondisk and incore fields, incore has preference.
+ * The inode must be kept locked SHARED by the caller.
+ */
+STATIC void
+xfs_ip_to_stat(
+	xfs_mount_t		*mp,
+	xfs_ino_t		ino,
+	xfs_inode_t		*ip,
+	dm_stat_t		*buf)
+{
+	xfs_icdinode_t		*dic = &ip->i_d;
+
+	buf->dt_ino = ino;
+	buf->dt_nlink = dic->di_nlink;
+	/*buf->dt_xfs_projid = dic->di_projid;*/
+	buf->dt_mode = dic->di_mode;
+	buf->dt_uid = dic->di_uid;
+	buf->dt_gid = dic->di_gid;
+	buf->dt_size = XFS_ISIZE(ip);
+	buf->dt_dev = new_encode_dev(mp->m_ddev_targp->bt_dev);
+	buf->dt_atime = VFS_I(ip)->i_atime.tv_sec;
+	buf->dt_mtime = dic->di_mtime.t_sec;
+	buf->dt_ctime = dic->di_ctime.t_sec;
+	buf->dt_xfs_xflags = xfs_ip2dmflags(ip);
+	buf->dt_xfs_extsize = dic->di_extsize << mp->m_sb.sb_blocklog;
+	buf->dt_xfs_extents = dic->di_nextents;
+	buf->dt_xfs_aextents = dic->di_anextents;
+	buf->dt_xfs_igen = dic->di_gen;
+	buf->dt_xfs_dmstate = dic->di_dmstate;
+
+	switch (dic->di_format) {
+	case XFS_DINODE_FMT_DEV:
+		buf->dt_rdev = ip->i_df.if_u2.if_rdev;
+		buf->dt_blksize = BLKDEV_IOSIZE;
+		buf->dt_blocks = 0;
+		break;
+	case XFS_DINODE_FMT_LOCAL:
+	case XFS_DINODE_FMT_UUID:
+		buf->dt_rdev = 0;
+		buf->dt_blksize = mp->m_sb.sb_blocksize;
+		buf->dt_blocks = 0;
+		break;
+	case XFS_DINODE_FMT_EXTENTS:
+	case XFS_DINODE_FMT_BTREE:
+		buf->dt_rdev = 0;
+		buf->dt_blksize = mp->m_sb.sb_blocksize;
+		buf->dt_blocks = XFS_FSB_TO_BB(mp,
+				(dic->di_nblocks + ip->i_delayed_blks));
+		break;
+	}
+
+	memset(&buf->dt_pad1, 0, sizeof(buf->dt_pad1));
+	memset(&buf->dt_pad2, 0, sizeof(buf->dt_pad2));
+	memset(&buf->dt_pad3, 0, sizeof(buf->dt_pad3));
+
+	/* Finally fill in the DMAPI specific fields */
+	buf->dt_pers = 0;
+	buf->dt_change = 0;
+	buf->dt_nevents = DM_EVENT_MAX;
+	buf->dt_emask = dic->di_dmevmask;
+	buf->dt_dtime = dic->di_ctime.t_sec;
+	/* Set if one of READ, WRITE or TRUNCATE bits is set in emask */
+	buf->dt_pmanreg = (DMEV_ISSET(DM_EVENT_READ, buf->dt_emask) ||
+			DMEV_ISSET(DM_EVENT_WRITE, buf->dt_emask) ||
+			DMEV_ISSET(DM_EVENT_TRUNCATE, buf->dt_emask)) ? 1 : 0;
+}
+
+/*
+ * Take the handle and put it at the end of a dm_xstat buffer.
+ * dt_compname is unused in bulkstat - so we zero it out.
+ * Finally, update link in dm_xstat_t to point to next struct.
+ */
+STATIC void
+xfs_dm_handle_to_xstat(
+	dm_xstat_t	*xbuf,
+	size_t		xstat_sz,
+	dm_handle_t	*handle,
+	size_t		handle_sz)
+{
+	dm_stat_t	*sbuf = &xbuf->dx_statinfo;
+
+	memcpy(xbuf + 1, handle, handle_sz);
+	sbuf->dt_handle.vd_offset = (ssize_t) sizeof(dm_xstat_t);
+	sbuf->dt_handle.vd_length = (size_t) DM_HSIZE(*handle);
+	memset(&sbuf->dt_compname, 0, sizeof(dm_vardata_t));
+	sbuf->_link = xstat_sz;
+}
+
+STATIC int
+xfs_dm_bulkall_iget_one(
+	xfs_mount_t	*mp,
+	xfs_ino_t	ino,
+	int		*value_lenp,
+	dm_xstat_t	*xbuf,
+	u_int		*xstat_szp,
+	char		*attr_name,
+	caddr_t		attr_buf)
+{
+	xfs_inode_t	*ip;
+	dm_handle_t	handle;
+	u_int		xstat_sz = *xstat_szp;
+	int		value_len = *value_lenp;
+	int		error;
+
+	error = xfs_iget(mp, NULL, ino,
+			 XFS_IGET_UNTRUSTED, XFS_ILOCK_SHARED, &ip);
+	if (error)
+		return error;
+
+	xfs_ip_to_stat(mp, ino, ip, &xbuf->dx_statinfo);
+	dm_ip_to_handle(VFS_I(ip), &handle);
+	xfs_dm_handle_to_xstat(xbuf, xstat_sz, &handle, sizeof(handle));
+
+	/* Drop ILOCK_SHARED for call to xfs_attr_get */
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+
+	memset(&xbuf->dx_attrdata, 0, sizeof(dm_vardata_t));
+	error = xfs_attr_get(ip, attr_name, attr_buf, &value_len, ATTR_ROOT);
+	iput(VFS_I(ip));
+
+	DM_EA_XLATE_ERR(error);
+	if (error && (error != -ENOATTR)) {
+		if (error == -E2BIG)
+			error = -ENOMEM;
+		return error;
+	}
+
+	/* How much space was in the attr? */
+	if (error != -ENOATTR) {
+		xbuf->dx_attrdata.vd_offset = xstat_sz;
+		xbuf->dx_attrdata.vd_length = value_len;
+		xstat_sz += (value_len+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+	}
+	*xstat_szp = xbuf->dx_statinfo._link = xstat_sz;
+	*value_lenp = value_len;
+	return 0;
+}
+
+/*
+ * This is used by dm_get_bulkall().
+ * Given a inumber, it igets the inode and fills the given buffer
+ * with the dm_xstat structure for the file.
+ */
+STATIC int
+xfs_dm_bulkall_one(
+	xfs_mount_t	*mp,		/* mount point for filesystem */
+	xfs_ino_t	ino,		/* inode number to get data for */
+	void		__user *buffer,	/* buffer to place output in */
+	int		ubsize,		/* size of buffer */
+	void		*private_data,	/* my private data */
+	int		*ubused,	/* amount of buffer we used */
+	int		*res)		/* bulkstat result code */
+{
+	dm_xstat_t	*xbuf;
+	u_int		xstat_sz;
+	int		error;
+	int		value_len;
+	int		kern_buf_sz;
+	int		attr_buf_sz;
+	caddr_t		attr_buf;
+	void __user	*attr_user_buf;
+	dm_bulkstat_one_t *dmb = (dm_bulkstat_one_t*)private_data;
+
+	*res = BULKSTAT_RV_NOTHING;
+
+	if (!buffer || xfs_internal_inum(mp, ino))
+		return -EINVAL;
+
+	xstat_sz = DM_STAT_SIZE(*xbuf, 0);
+	xstat_sz = (xstat_sz + (DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+	if (xstat_sz > ubsize)
+		return -ENOMEM;
+
+	kern_buf_sz = xstat_sz;
+	xbuf = kmem_zalloc_large(kern_buf_sz, KM_SLEEP);
+
+	/* Determine place to drop attr value, and available space. */
+	value_len = ubsize - xstat_sz;
+	if (value_len > ATTR_MAX_VALUELEN)
+		value_len = ATTR_MAX_VALUELEN;
+
+	attr_user_buf = buffer + xstat_sz;
+	attr_buf_sz = value_len;
+	attr_buf = kmem_zalloc_large(attr_buf_sz, KM_SLEEP);
+
+	error = xfs_dm_bulkall_iget_one(mp, ino,
+					&value_len, xbuf, &xstat_sz,
+					dmb->attrname.dan_chars,
+					attr_buf);
+	if (error)
+		goto out_free_buffers;
+
+	if (copy_to_user(buffer, xbuf, kern_buf_sz)) {
+		error = -EFAULT;
+		goto out_free_buffers;
+	}
+	if (copy_to_user(attr_user_buf, attr_buf, value_len)) {
+		error = -EFAULT;
+		goto out_free_buffers;
+	}
+
+	kmem_free(attr_buf);
+	kmem_free(xbuf);
+
+	*res = BULKSTAT_RV_DIDONE;
+	if (ubused)
+		*ubused = xstat_sz;
+	dmb->laststruct = buffer;
+	return 0;
+
+ out_free_buffers:
+	kmem_free(attr_buf);
+	kmem_free(xbuf);
+	return error;
+}
+
+/*
+ * Take the handle and put it at the end of a dm_stat buffer.
+ * dt_compname is unused in bulkstat - so we zero it out.
+ * Finally, update link in dm_stat_t to point to next struct.
+ */
+STATIC void
+xfs_dm_handle_to_stat(
+	dm_stat_t	*sbuf,
+	size_t		stat_sz,
+	dm_handle_t	*handle,
+	size_t		handle_sz)
+{
+	memcpy(sbuf + 1, handle, handle_sz);
+	sbuf->dt_handle.vd_offset = (ssize_t) sizeof(dm_stat_t);
+	sbuf->dt_handle.vd_length = (size_t) DM_HSIZE(*handle);
+	memset(&sbuf->dt_compname, 0, sizeof(dm_vardata_t));
+	sbuf->_link = stat_sz;
+}
+
+STATIC int
+xfs_dm_bulkattr_iget_one(
+	xfs_mount_t	*mp,
+	xfs_ino_t	ino,
+	dm_stat_t	*sbuf,
+	u_int		stat_sz)
+{
+	xfs_inode_t	*ip;
+	dm_handle_t	handle;
+	int		error;
+
+	error = xfs_iget(mp, NULL, ino,
+			 XFS_IGET_UNTRUSTED, XFS_ILOCK_SHARED, &ip);
+	if (error)
+		return error;
+
+	xfs_ip_to_stat(mp, ino, ip, sbuf);
+	dm_ip_to_handle(VFS_I(ip), &handle);
+	xfs_dm_handle_to_stat(sbuf, stat_sz, &handle, sizeof(handle));
+
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+	IRELE(ip);
+	return 0;
+}
+
+/*
+ * This is used by dm_get_bulkattr().
+ * Given a inumber, it igets the inode and fills the given buffer
+ * with the dm_stat structure for the file.
+ */
+STATIC int
+xfs_dm_bulkattr_one(
+	xfs_mount_t	*mp,		/* mount point for filesystem */
+	xfs_ino_t	ino,		/* inode number to get data for */
+	void		__user *buffer,	/* buffer to place output in */
+	int		ubsize,		/* size of buffer */
+	void		*private_data,	/* my private data */
+	int		*ubused,	/* amount of buffer we used */
+	int		*res)		/* bulkstat result code */
+{
+	dm_stat_t	*sbuf;
+	u_int		stat_sz;
+	int		error;
+	dm_bulkstat_one_t *dmb = (dm_bulkstat_one_t*)private_data;
+
+	ASSERT( private_data != NULL );
+	*res = BULKSTAT_RV_NOTHING;
+
+	if (!buffer || xfs_internal_inum(mp, ino))
+		return -EINVAL;
+
+	stat_sz = DM_STAT_SIZE(*sbuf, 0);
+	stat_sz = (stat_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+	if (stat_sz > ubsize)
+		return -ENOMEM;
+
+	sbuf = kmem_zalloc_large(stat_sz, KM_SLEEP);
+
+	error = xfs_dm_bulkattr_iget_one(mp, ino, sbuf, stat_sz);
+	if (error)
+		goto out_free_buffer;
+
+	if (copy_to_user(buffer, sbuf, stat_sz)) {
+		error = -EFAULT;
+		goto out_free_buffer;
+	}
+
+	kmem_free(sbuf);
+	*res = BULKSTAT_RV_DIDONE;
+	if (ubused)
+		*ubused = stat_sz;
+	dmb->laststruct = buffer;
+	return 0;
+
+ out_free_buffer:
+	kmem_free(sbuf);
+	return error;
+}
+
+/* xfs_dm_f_get_eventlist - return the dm_eventset_t mask for inode ip. */
+
+STATIC int
+xfs_dm_f_get_eventlist(
+	xfs_inode_t	*ip,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,		/* in kernel space! */
+	u_int		*nelemp)		/* in kernel space! */
+{
+	dm_eventset_t	eventset;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	/* Note that we MUST return a regular file's managed region bits as
+	   part of the mask because dm_get_eventlist is supposed to return the
+	   union of all managed region flags in those bits.  Since we only
+	   support one region, we can just return the bits as they are.	 For
+	   all other object types, the bits will already be zero.  Handy, huh?
+	*/
+
+	eventset = ip->i_d.di_dmevmask;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	*eventsetp = eventset;
+	*nelemp = nelem;
+	return 0;
+}
+
+
+/* xfs_dm_f_set_eventlist - update the dm_eventset_t mask in the inode vp.  Only the
+   bits from zero to maxevent-1 are being replaced; higher bits are preserved.
+*/
+
+STATIC int
+xfs_dm_f_set_eventlist(
+	xfs_inode_t	*ip,
+	dm_right_t	right,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	dm_eventset_t	eventset;
+	dm_eventset_t	max_mask;
+	dm_eventset_t	valid_events;
+	xfs_trans_t	*tp;
+	xfs_mount_t	*mp;
+	struct xfs_trans_res	tres;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	eventset = *eventsetp;
+	if (maxevent >= sizeof(ip->i_d.di_dmevmask) * NBBY)
+		return -EINVAL;
+	max_mask = (1 << maxevent) - 1;
+
+	if (S_ISDIR(ip->i_d.di_mode)) {
+		valid_events = DM_XFS_VALID_DIRECTORY_EVENTS;
+	} else {	/* file or symlink */
+		valid_events = DM_XFS_VALID_FILE_EVENTS;
+	}
+	if ((eventset & max_mask) & ~valid_events)
+		return -EINVAL;
+
+	/* Adjust the event mask so that the managed region bits will not
+	   be altered.
+	*/
+
+	max_mask &= ~(1 <<DM_EVENT_READ);	/* preserve current MR bits */
+	max_mask &= ~(1 <<DM_EVENT_WRITE);
+	max_mask &= ~(1 <<DM_EVENT_TRUNCATE);
+
+	mp = ip->i_mount;
+	tres.tr_logres = M_RES(mp)->tr_ichange.tr_logres;
+	tres.tr_logcount = 0;
+	tres.tr_logflags = 0;
+	tp = xfs_trans_alloc(mp, XFS_TRANS_SET_DMATTRS);
+	error = xfs_trans_reserve(tp, &tres, 0, 0);
+	if (error) {
+		xfs_trans_cancel(tp);
+		return error;
+	}
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+
+	ip->i_d.di_dmevmask = (eventset & max_mask) | (ip->i_d.di_dmevmask & ~max_mask);
+
+	xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+	xfs_trans_commit(tp);
+
+	return 0;
+}
+
+
+/* xfs_dm_fs_get_eventlist - return the dm_eventset_t mask for filesystem vfsp. */
+
+STATIC int
+xfs_dm_fs_get_eventlist(
+	xfs_mount_t	*mp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,		/* in kernel space! */
+	u_int		*nelemp)		/* in kernel space! */
+{
+	dm_eventset_t	eventset;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	eventset = mp->m_dmevmask;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	*eventsetp = eventset;
+	*nelemp = nelem;
+	return 0;
+}
+
+
+/* xfs_dm_fs_set_eventlist - update the dm_eventset_t mask in the mount structure for
+   filesystem vfsp.  Only the bits from zero to maxevent-1 are being replaced;
+   higher bits are preserved.
+*/
+
+STATIC int
+xfs_dm_fs_set_eventlist(
+	xfs_mount_t	*mp,
+	dm_right_t	right,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	dm_eventset_t	eventset;
+	dm_eventset_t	max_mask;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	eventset = *eventsetp;
+
+	if (maxevent >= sizeof(mp->m_dmevmask) * NBBY)
+		return -EINVAL;
+	max_mask = (1 << maxevent) - 1;
+
+	if ((eventset & max_mask) & ~DM_XFS_VALID_FS_EVENTS)
+		return -EINVAL;
+
+	mp->m_dmevmask = (eventset & max_mask) | (mp->m_dmevmask & ~max_mask);
+	return 0;
+}
+
+
+/* Code in this routine must exactly match the logic in xfs_diordwr() in
+   order for this to work!
+*/
+
+STATIC int
+xfs_dm_direct_ok(
+	xfs_inode_t	*ip,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		__user *bufp)
+{
+	xfs_mount_t	*mp;
+
+	mp = ip->i_mount;
+
+	/* Realtime files can ONLY do direct I/O. */
+
+	if (XFS_IS_REALTIME_INODE(ip))
+		return 1;
+
+	/* If direct I/O is disabled, or if the request is too small, use
+	   buffered I/O.
+	*/
+
+	if (!dm_min_dio_xfer || len < dm_min_dio_xfer)
+		return 0;
+
+#if 0
+	/* If the request is not well-formed or is too large, use
+	   buffered I/O.
+	*/
+
+	if ((uintptr_t)bufp & scache_linemask)	/* if buffer not aligned */
+		return 0;
+	if (off & mp->m_blockmask)		/* if file offset not aligned */
+		return 0;
+	if (len & mp->m_blockmask)		/* if xfer length not aligned */
+		return 0;
+	if (len > ctooff(v.v_maxdmasz - 1))	/* if transfer too large */
+		return 0;
+
+	/* A valid direct I/O candidate. */
+
+	return 1;
+#else
+	return 0;
+#endif
+}
+
+/* new_sync_read */
+static ssize_t xfs_dm_read_iter(struct file *filp, char __user *buf, size_t len,
+				loff_t *ppos)
+{
+	struct iovec iov = { .iov_base = buf, .iov_len = len };
+	struct kiocb kiocb;
+	struct iov_iter iter;
+	ssize_t ret;
+
+	init_sync_kiocb(&kiocb, filp);
+	kiocb.ki_pos = *ppos;
+	iov_iter_init(&iter, READ, &iov, 1, len);
+
+	ret = filp->f_op->read_iter(&kiocb, &iter);
+	BUG_ON(ret == -EIOCBQUEUED);
+	*ppos = kiocb.ki_pos;
+	return ret;
+}
+
+static ssize_t xfs_dm_write_iter(struct file *filp, const char __user *buf,
+				 size_t len, loff_t *ppos)
+{
+	struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = len };
+	struct kiocb kiocb;
+	struct iov_iter iter;
+	ssize_t ret;
+
+	init_sync_kiocb(&kiocb, filp);
+	kiocb.ki_pos = *ppos;
+	iov_iter_init(&iter, WRITE, &iov, 1, len);
+
+	ret = filp->f_op->write_iter(&kiocb, &iter);
+	BUG_ON(ret == -EIOCBQUEUED);
+	if (ret > 0)
+		*ppos = kiocb.ki_pos;
+	return ret;
+}
+/* We need to be able to select various combinations of O_NONBLOCK,
+   O_DIRECT, and O_SYNC, yet we don't have a file descriptor and we don't have
+   the file's pathname.	 All we have is a handle.
+*/
+
+STATIC int
+xfs_dm_rdwr(
+	struct inode	*inode,
+	uint		fflag,
+	mode_t		fmode,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		__user *bufp,
+	int		*rvp)
+{
+	const struct cred *cred = current_cred();
+	xfs_inode_t	*ip = XFS_I(inode);
+	int		error;
+	int		oflags;
+	ssize_t		xfer;
+	struct file	*file;
+	struct dentry	*dentry;
+	struct path	path;
+	struct path	dpath;
+	struct xfs_mount *mp = ip->i_mount;
+
+	if ((off < 0) || (off > i_size_read(inode)) || !S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (fmode & FMODE_READ) {
+		oflags = O_RDONLY;
+	} else {
+		oflags = O_WRONLY;
+	}
+
+	/*
+	 * Build file descriptor flags and I/O flags.  O_NONBLOCK is needed so
+	 * that we don't block on mandatory file locks. This is an invisible IO,
+	 * don't change the atime.
+	 */
+
+	oflags |= O_LARGEFILE | O_NONBLOCK | O_NOATIME;
+	if (xfs_dm_direct_ok(ip, off, len, bufp))
+		oflags |= O_DIRECT;
+
+	if (fflag & O_SYNC)
+		oflags |= O_SYNC;
+
+	if (inode->i_fop == NULL) {
+		/* no iput; caller did get, and will do put */
+		return -EINVAL;
+	}
+
+	igrab(inode);
+
+	dentry = d_obtain_alias(inode);
+	if (dentry == NULL) {
+		iput(inode);
+		return -ENOMEM;
+	}
+
+	error = kern_path(mp->m_mtpt, 0, &path);
+	if (error)
+		return error;
+
+	/* create a path structure for the inode passed into function */
+	dpath.mnt = path.mnt;
+	dpath.dentry = dentry;
+	file = dentry_open(&dpath, oflags, cred);
+	/* path_put() will only dput the directory dentry, so we do it here */
+	dput(dentry);
+	if (IS_ERR(file)) {
+		path_put(&path);
+		return PTR_ERR(file);
+	}
+	file->f_mode |= FMODE_NOCMTIME;
+
+	if (fmode & FMODE_READ && file->f_op->read != NULL) {
+		xfer = file->f_op->read(file, bufp, len, (loff_t*)&off);
+	} else if (fmode & FMODE_READ && file->f_op->read_iter != NULL) {
+		xfer = xfs_dm_read_iter(file, bufp, len, (loff_t*)&off);
+	} else if (fmode & FMODE_WRITE && file->f_op->write != NULL) {
+		xfer = file->f_op->write(file, bufp, len, (loff_t*)&off);
+	} else if (fmode & FMODE_WRITE && file->f_op->write_iter != NULL) {
+		xfer = xfs_dm_write_iter(file, bufp, len, (loff_t*)&off);
+	} else
+		xfer = -EINVAL; /* error code when f_op functions are NULL */
+
+	if (xfer >= 0) {
+		*rvp = xfer;
+		error = 0;
+	} else {
+		error = (int)xfer;
+	}
+
+	fput(file);
+	path_put(&path);
+	return error;
+}
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_clear_inherit(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrname_t	__user *attrnamep)
+{
+	return -ENOSYS;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_create_by_handle(
+	struct inode	*inode,
+	dm_right_t	right,
+	void		__user *hanp,
+	size_t		hlen,
+	char		__user *cname)
+{
+	return -ENOSYS;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_downgrade_right(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(dm_handle_t) * 2 + 1];
+
+	if (!xfs_vp_to_hexhandle(inode, type, buffer)) {
+		printf("dm_downgrade_right: old %d new %d type %d handle %s\n",
+			right, DM_RIGHT_SHARED, type, buffer);
+	} else {
+		printf("dm_downgrade_right: old %d new %d type %d handle "
+			"<INVALID>\n", right, DM_RIGHT_SHARED, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return 0;
+}
+
+
+/* Note: xfs_dm_get_allocinfo() makes no attempt to coalesce two adjacent
+   extents when both are of type DM_EXTENT_RES; this is left to the caller.
+   XFS guarantees that there will never be two adjacent DM_EXTENT_HOLE extents.
+
+   In order to provide the caller with all extents in a file including
+   those beyond the file's last byte offset, we have to use the xfs_bmapi()
+   interface.
+*/
+
+STATIC int
+xfs_dm_get_allocinfo_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_off_t	__user	*offp,
+	u_int		nelem,
+	dm_extent_t	__user *extentp,
+	u_int		__user *nelemp,
+	int		*rvp)
+{
+	xfs_inode_t	*ip = XFS_I(inode);
+	xfs_mount_t	*mp;		/* file system mount point */
+	xfs_fileoff_t	fsb_offset;
+	xfs_filblks_t	fsb_length;
+	dm_off_t	startoff;
+	int		elem;
+	xfs_bmbt_irec_t *bmp = NULL;
+	u_int		bmpcnt = 50;
+	u_int		bmpsz = sizeof(xfs_bmbt_irec_t) * bmpcnt;
+	int		error = 0;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	if ((inode->i_mode & S_IFMT) != S_IFREG)
+		return -EINVAL;
+
+	if (copy_from_user( &startoff, offp, sizeof(startoff)))
+		return -EFAULT;
+
+	mp = ip->i_mount;
+	ASSERT(mp);
+
+	if (startoff > mp->m_super->s_maxbytes)
+		return -EINVAL;
+
+	if (nelem == 0)
+		return -EINVAL;
+
+	/* Convert the caller's starting offset into filesystem allocation
+	   units as required by xfs_bmapi().  Round the offset down so that
+	   it is sure to be included in the reply.
+	*/
+
+	fsb_offset = XFS_B_TO_FSBT(mp, startoff);
+	fsb_length = XFS_B_TO_FSB(mp, mp->m_super->s_maxbytes) - fsb_offset;
+	elem = 0;
+
+	if (fsb_length)
+		bmp = kmem_zalloc_large(bmpsz, KM_SLEEP);
+
+	while (fsb_length && elem < nelem) {
+		dm_extent_t	extent;
+		xfs_filblks_t	fsb_bias;
+		dm_size_t	bias;
+		int		lock;
+		int		num;
+		int		i;
+
+		/* Compute how many getbmap structures to use on the xfs_bmapi
+		   call.
+		*/
+
+		num = MIN((u_int)(nelem - elem), bmpcnt);
+
+		xfs_ilock(ip, XFS_IOLOCK_SHARED);
+		lock = xfs_ilock_data_map_shared(ip);
+
+		error = xfs_bmapi_read(ip, fsb_offset, fsb_length,
+				       bmp, &num, XFS_BMAPI_ENTIRE);
+		xfs_iunlock(ip, lock);
+		xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+
+		if (error)
+			goto finish_out;
+
+		/* Fill in the caller's extents, adjusting the bias in the
+		   first entry if necessary.
+		*/
+
+		for (i = 0; i < num; i++, extentp++) {
+			bias = startoff - XFS_FSB_TO_B(mp, bmp[i].br_startoff);
+			extent.ex_offset = startoff;
+			extent.ex_length =
+				XFS_FSB_TO_B(mp, bmp[i].br_blockcount) - bias;
+			if (bmp[i].br_startblock == HOLESTARTBLOCK) {
+				extent.ex_type = DM_EXTENT_HOLE;
+			} else {
+				extent.ex_type = DM_EXTENT_RES;
+			}
+			startoff = extent.ex_offset + extent.ex_length;
+
+			if (copy_to_user( extentp, &extent, sizeof(extent))) {
+				error = -EFAULT;
+				goto finish_out;
+			}
+
+			fsb_bias = fsb_offset - bmp[i].br_startoff;
+			fsb_offset += bmp[i].br_blockcount - fsb_bias;
+			fsb_length -= bmp[i].br_blockcount - fsb_bias;
+			elem++;
+		}
+	}
+
+	if (fsb_length == 0) {
+		startoff = 0;
+	}
+	if (copy_to_user( offp, &startoff, sizeof(startoff))) {
+		error = -EFAULT;
+		goto finish_out;
+	}
+
+	if (copy_to_user( nelemp, &elem, sizeof(elem))) {
+		error = -EFAULT;
+		goto finish_out;
+	}
+
+	*rvp = (fsb_length == 0 ? 0 : 1);
+
+finish_out:
+	if (bmp)
+		kmem_free(bmp);
+	return error;
+}
+
+
+STATIC int
+xfs_dm_zero_xstatinfo_link(
+	dm_xstat_t __user	*dxs)
+{
+	dm_xstat_t		*ldxs;
+	int			error = 0;
+
+	if (!dxs)
+		return 0;
+	ldxs = kmalloc(sizeof(*ldxs), GFP_KERNEL);
+	if (!ldxs)
+		return -ENOMEM;
+	if (copy_from_user(ldxs, dxs, sizeof(*dxs))) {
+		error = -EFAULT;
+	} else {
+		ldxs->dx_statinfo._link = 0;
+		if (copy_to_user(dxs, ldxs, sizeof(*dxs)))
+			error = -EFAULT;
+	}
+	kfree(ldxs);
+	return error;
+}
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_bulkall_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrname_t	__user *attrnamep,
+	dm_attrloc_t	__user *locp,
+	size_t		buflen,
+	void		__user *bufp,	/* address of buffer in user space */
+	size_t		__user *rlenp,	/* user space address */
+	int		*rvalp)
+{
+	int		error, done;
+	int		nelems;
+	u_int		statstruct_sz;
+	dm_attrloc_t	loc;
+	xfs_mount_t	*mp = XFS_I(inode)->i_mount;
+	dm_attrname_t	attrname;
+	dm_bulkstat_one_t dmb;
+
+	if (copy_from_user(&attrname, attrnamep, sizeof(attrname)) ||
+	    copy_from_user(&loc, locp, sizeof(loc)))
+		return -EFAULT;
+
+	if (attrname.an_chars[0] == '\0')
+		return -EINVAL;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	/* Because we will write directly to the user's buffer, make sure that
+	   the buffer is properly aligned.
+	*/
+
+	if (((unsigned long)bufp & (DM_STAT_ALIGN - 1)) != 0)
+		return -EFAULT;
+
+	/* Size of the handle is constant for this function.
+	 * If there are no files with attributes, then this will be the
+	 * maximum number of inodes we can get.
+	 */
+
+	statstruct_sz = DM_STAT_SIZE(dm_xstat_t, 0);
+	statstruct_sz = (statstruct_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+
+	nelems = buflen / statstruct_sz;
+	if (nelems < 1) {
+		if (put_user( statstruct_sz, rlenp ))
+			return -EFAULT;
+		return -E2BIG;
+	}
+
+	/* Build the on-disk version of the attribute name. */
+	strcpy(dmb.attrname.dan_chars, dmattr_prefix);
+	strncpy(&dmb.attrname.dan_chars[DMATTR_PREFIXLEN],
+		attrname.an_chars, DM_ATTR_NAME_SIZE + 1);
+	dmb.attrname.dan_chars[sizeof(dmb.attrname.dan_chars) - 1] = '\0';
+
+	/*
+	 * fill the buffer with dm_xstat_t's
+	 */
+
+	dmb.laststruct = NULL;
+	memcpy(&dmb.fsid, mp->m_fixedfsid, sizeof(dm_fsid_t));
+	error = xfs_bulkstat(mp, (xfs_ino_t *)&loc, &nelems,
+			     xfs_dm_bulkall_one, (void*)&dmb, statstruct_sz,
+			     bufp, &done);
+	if (error)
+		return error;
+
+	*rvalp = !done ? 1 : 0;
+
+	if (put_user( statstruct_sz * nelems, rlenp ))
+		return -EFAULT;
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return -EFAULT;
+	/*
+	 *  If we didn't do any, we must not have any more to do.
+	 */
+	if (nelems < 1)
+		return 0;
+	/*
+	 * Set _link in the last struct to zero
+	 */
+	return xfs_dm_zero_xstatinfo_link((dm_xstat_t __user *)dmb.laststruct);
+}
+
+
+STATIC int
+xfs_dm_zero_statinfo_link(
+	dm_stat_t __user	*dxs)
+{
+	dm_stat_t		*ldxs;
+	int			error = 0;
+
+	if (!dxs)
+		return 0;
+	ldxs = kmalloc(sizeof(*ldxs), GFP_KERNEL);
+	if (!ldxs)
+		return -ENOMEM;
+	if (copy_from_user(ldxs, dxs, sizeof(*dxs))) {
+		error = -EFAULT;
+	} else {
+		ldxs->_link = 0;
+		if (copy_to_user(dxs, ldxs, sizeof(*dxs)))
+			error = -EFAULT;
+	}
+	kfree(ldxs);
+	return error;
+}
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_bulkattr_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrloc_t	__user *locp,
+	size_t		buflen,
+	void		__user *bufp,
+	size_t		__user *rlenp,
+	int		*rvalp)
+{
+	int		error, done;
+	int		nelems;
+	u_int		statstruct_sz;
+	dm_attrloc_t	loc;
+	xfs_mount_t	*mp = XFS_I(inode)->i_mount;
+	dm_bulkstat_one_t dmb;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	if (copy_from_user( &loc, locp, sizeof(loc)))
+		return -EFAULT;
+
+	/* Because we will write directly to the user's buffer, make sure that
+	   the buffer is properly aligned.
+	*/
+
+	if (((unsigned long)bufp & (DM_STAT_ALIGN - 1)) != 0)
+		return -EFAULT;
+
+	/* size of the handle is constant for this function */
+
+	statstruct_sz = DM_STAT_SIZE(dm_stat_t, 0);
+	statstruct_sz = (statstruct_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+
+	nelems = buflen / statstruct_sz;
+	if (nelems < 1) {
+		if (put_user( statstruct_sz, rlenp ))
+			return -EFAULT;
+		return -E2BIG;
+	}
+
+	dmb.laststruct = NULL;
+	memcpy(&dmb.fsid, mp->m_fixedfsid, sizeof(dm_fsid_t));
+	error = xfs_bulkstat(mp, (xfs_ino_t *)&loc, &nelems,
+				xfs_dm_bulkattr_one, (void*)&dmb,
+				statstruct_sz, bufp, &done);
+	if (error)
+		return error;
+
+	*rvalp = !done ? 1 : 0;
+
+	if (put_user( statstruct_sz * nelems, rlenp ))
+		return -EFAULT;
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return -EFAULT;
+
+	/*
+	 *  If we didn't do any, we must not have any more to do.
+	 */
+	if (nelems < 1)
+		return 0;
+	/*
+	 * Set _link in the last struct to zero
+	 */
+	return xfs_dm_zero_statinfo_link((dm_stat_t __user *)dmb.laststruct);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_config(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_config_t	flagname,
+	dm_size_t	__user *retvalp)
+{
+	dm_size_t	retval;
+
+	switch (flagname) {
+	case DM_CONFIG_DTIME_OVERLOAD:
+	case DM_CONFIG_PERS_ATTRIBUTES:
+	case DM_CONFIG_PERS_EVENTS:
+	case DM_CONFIG_PERS_MANAGED_REGIONS:
+	case DM_CONFIG_PUNCH_HOLE:
+	case DM_CONFIG_WILL_RETRY:
+		retval = DM_TRUE;
+		break;
+
+	case DM_CONFIG_CREATE_BY_HANDLE:	/* these will never be done */
+	case DM_CONFIG_LOCK_UPGRADE:
+	case DM_CONFIG_PERS_INHERIT_ATTRIBS:
+		retval = DM_FALSE;
+		break;
+
+	case DM_CONFIG_BULKALL:
+		retval = DM_TRUE;
+		break;
+	case DM_CONFIG_MAX_ATTR_ON_DESTROY:
+		retval = DM_MAX_ATTR_BYTES_ON_DESTROY;
+		break;
+
+	case DM_CONFIG_MAX_ATTRIBUTE_SIZE:
+		retval = ATTR_MAX_VALUELEN;
+		break;
+
+	case DM_CONFIG_MAX_HANDLE_SIZE:
+		retval = DM_MAX_HANDLE_SIZE;
+		break;
+
+	case DM_CONFIG_MAX_MANAGED_REGIONS:
+		retval = 1;
+		break;
+
+	case DM_CONFIG_TOTAL_ATTRIBUTE_SPACE:
+		retval = 0x7fffffff;	/* actually it's unlimited */
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	/* Copy the results back to the user. */
+
+	if (copy_to_user( retvalp, &retval, sizeof(retval)))
+		return -EFAULT;
+	return 0;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_config_events(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	__user *eventsetp,
+	u_int		__user *nelemp)
+{
+	dm_eventset_t	eventset;
+
+	if (nelem == 0)
+		return -EINVAL;
+
+	eventset = DM_XFS_SUPPORTED_EVENTS;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	if (copy_to_user( eventsetp, &eventset, sizeof(eventset)))
+		return -EFAULT;
+
+	if (put_user(nelem, nelemp))
+		return -EFAULT;
+	return 0;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_destroy_dmattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrname_t  *attrnamep,
+	char 		**valuepp,
+	int		*vlenp)
+{
+	dm_dkattrname_t dkattrname;
+	int		alloc_size;
+	int		value_len;
+	char		*value;
+	int		error;
+
+	*vlenp = -1;		/* assume failure by default */
+
+	if (attrnamep->an_chars[0] == '\0')
+		return -EINVAL;
+
+	/* Build the on-disk version of the attribute name. */
+
+	strcpy(dkattrname.dan_chars, dmattr_prefix);
+	strncpy(&dkattrname.dan_chars[DMATTR_PREFIXLEN],
+		(char *)attrnamep->an_chars, DM_ATTR_NAME_SIZE + 1);
+	dkattrname.dan_chars[sizeof(dkattrname.dan_chars) - 1] = '\0';
+
+	/* xfs_attr_get will not return anything if the buffer is too small,
+	   and we don't know how big to make the buffer, so this may take
+	   two tries to get it right.  The initial try must use a buffer of
+	   at least XFS_BUG_KLUDGE bytes to prevent buffer overflow because
+	   of a bug in XFS.
+	*/
+
+	alloc_size = XFS_BUG_KLUDGE;
+	value = kmalloc(alloc_size, GFP_KERNEL);
+	if (value == NULL)
+		return -ENOMEM;
+
+	error = xfs_attr_get(XFS_I(inode), dkattrname.dan_chars, value,
+							&value_len, ATTR_ROOT);
+	if (error == -ERANGE) {
+		kfree(value);
+		alloc_size = value_len;
+		value = kmalloc(alloc_size, GFP_KERNEL);
+		if (value == NULL)
+			return -ENOMEM;
+
+		error = xfs_attr_get(XFS_I(inode), dkattrname.dan_chars, value,
+					&value_len, ATTR_ROOT);
+	}
+	if (error) {
+		kfree(value);
+		DM_EA_XLATE_ERR(error);
+		return error;
+	}
+
+	/* The attribute exists and has a value.  Note that a value_len of
+	   zero is valid!
+	*/
+
+	if (value_len == 0) {
+		kfree(value);
+		*vlenp = 0;
+		return 0;
+	} else if (value_len > DM_MAX_ATTR_BYTES_ON_DESTROY) {
+		char	*value2;
+
+		value2 = kmalloc(DM_MAX_ATTR_BYTES_ON_DESTROY, GFP_KERNEL);
+		if (value2 == NULL) {
+			kfree(value);
+			return -ENOMEM;
+		}
+		memcpy(value2, value, DM_MAX_ATTR_BYTES_ON_DESTROY);
+		kfree(value);
+		value = value2;
+		value_len = DM_MAX_ATTR_BYTES_ON_DESTROY;
+	}
+	*vlenp = value_len;
+	*valuepp = value;
+	return 0;
+}
+
+/* This code was taken from xfs_fcntl(F_DIOINFO) and modified slightly because
+   we don't have a flags parameter (no open file).
+   Taken from xfs_ioctl(XFS_IOC_DIOINFO) on Linux.
+*/
+
+STATIC int
+xfs_dm_get_dioinfo(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_dioinfo_t	__user *diop)
+{
+	dm_dioinfo_t	dio;
+	xfs_mount_t	*mp;
+	xfs_inode_t	*ip = XFS_I(inode);
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	mp = ip->i_mount;
+
+	dio.d_miniosz = dio.d_mem = MIN_DIO_SIZE(mp);
+	dio.d_maxiosz = MAX_DIO_SIZE(mp);
+	dio.d_dio_only = DM_FALSE;
+
+	if (copy_to_user(diop, &dio, sizeof(dio)))
+		return -EFAULT;
+	return 0;
+}
+
+typedef struct dm_readdir_cb {
+	struct dir_context	ctx;
+	xfs_mount_t		*mp;
+	char __user		*ubuf;
+	dm_stat_t __user	*lastbuf;
+	size_t			spaceleft;
+	size_t			nwritten;
+	int			error;
+	dm_stat_t		*kstat;
+} dm_readdir_cb_t;
+
+STATIC int
+dm_filldir(struct dir_context *ctx, const char *name, int namelen,
+	   loff_t offset, u64 ino, unsigned int d_type)
+{
+	dm_readdir_cb_t *cb = container_of(ctx, struct dm_readdir_cb, ctx);
+	dm_stat_t	*statp = cb->kstat;
+	size_t		len;
+	int		error;
+	int		needed;
+
+	/*
+	 * Make sure we have enough space.
+	 */
+        needed = dm_stat_size(namelen + 1);
+	if (cb->spaceleft < needed) {
+		cb->spaceleft = 0;
+		return -ENOSPC;
+	}
+
+	error = -EINVAL;
+	if (xfs_internal_inum(cb->mp, ino))
+		goto out_err;
+
+	memset(statp, 0, dm_stat_size(MAXNAMLEN));
+	error = xfs_dm_bulkattr_iget_one(cb->mp, ino,
+			statp, needed);
+	if (error)
+		goto out_err;
+
+	/*
+	 * On return from bulkstat_one(), stap->_link points
+	 * at the end of the handle in the stat structure.
+	 */
+	statp->dt_compname.vd_offset = statp->_link;
+	statp->dt_compname.vd_length = namelen + 1;
+
+	len = statp->_link;
+
+	/* Word-align the record */
+	statp->_link = dm_stat_align(len + namelen + 1);
+
+	error = -EFAULT;
+	if (copy_to_user(cb->ubuf, statp, len))
+		goto out_err;
+	if (copy_to_user(cb->ubuf + len, name, namelen))
+		goto out_err;
+	if (put_user(0, cb->ubuf + len + namelen))
+		goto out_err;
+
+	cb->lastbuf = (dm_stat_t __user *)cb->ubuf;
+	cb->spaceleft -= statp->_link;
+	cb->nwritten += statp->_link;
+	cb->ubuf += statp->_link;
+
+	return 0;
+
+ out_err:
+	cb->error = error;
+	return error;
+}
+
+STATIC int
+xfs_dm_get_dirattrs_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrloc_t	__user *locp,
+	size_t		buflen,
+	void		__user *bufp,
+	size_t		__user *rlenp,
+	int		*rvp)
+{
+	xfs_inode_t	*dp = XFS_I(inode);
+	xfs_mount_t	*mp = dp->i_mount;
+	dm_readdir_cb_t	cb = { .ctx.actor = &dm_filldir, };
+	dm_attrloc_t	loc;
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+        /*
+         * Make sure that the buffer is properly aligned.
+         */
+        if (((unsigned long)bufp & (DM_STAT_ALIGN - 1)) != 0)
+                return -EFAULT;
+
+	if (mask & ~(DM_AT_HANDLE|DM_AT_EMASK|DM_AT_PMANR|DM_AT_PATTR|
+		     DM_AT_DTIME|DM_AT_CFLAG|DM_AT_STAT))
+		return -EINVAL;
+
+	if (!S_ISDIR(inode->i_mode))
+		return -EINVAL;
+
+        /*
+         * bufp should be able to fit at least one dm_stat entry including
+         * dt_handle and full size MAXNAMLEN dt_compname.
+         */
+        if (buflen < dm_stat_size(MAXNAMLEN))
+                return -ENOMEM;
+
+	if (copy_from_user(&loc, locp, sizeof(loc)))
+		return -EFAULT;
+
+	cb.kstat = kzalloc(dm_stat_size(MAXNAMLEN), GFP_KERNEL);
+	if (!cb.kstat)
+		return -ENOMEM;
+
+	cb.mp = mp;
+	cb.spaceleft = buflen;
+	cb.ubuf = bufp;
+	cb.ctx.pos = loc;
+	mutex_lock(&inode->i_mutex);
+	error = -ENOENT;
+	if (!IS_DEADDIR(inode)) {
+		error = xfs_readdir(dp, &cb.ctx, buflen);
+	}
+	mutex_unlock(&inode->i_mutex);
+	loc = cb.ctx.pos;
+
+	if (error)
+		goto out_kfree;
+	if (cb.error) {
+		error = cb.error;
+		goto out_kfree;
+	}
+
+	error = -EFAULT;
+	if (cb.lastbuf && put_user(0, &cb.lastbuf->_link))
+		goto out_kfree;
+	if (put_user(cb.nwritten, rlenp))
+		goto out_kfree;
+	if (copy_to_user(locp, &loc, sizeof(loc)))
+		goto out_kfree;
+
+	if (cb.nwritten)
+		*rvp = 1;
+	else
+		*rvp = 0;
+	error = 0;
+
+ out_kfree:
+	kfree(cb.kstat);
+	return error;
+}
+
+STATIC int
+xfs_dm_get_dmattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrname_t	__user *attrnamep,
+	size_t		buflen,
+	void		__user *bufp,
+	size_t		__user  *rlenp)
+{
+	dm_dkattrname_t name;
+	char		*value;
+	int		value_len;
+	int		alloc_size;
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return error;
+
+	/* Allocate a buffer to receive the attribute's value.	We allocate
+	   at least one byte even if the caller specified a buflen of zero.
+	   (A buflen of zero is considered valid.)
+
+	   Allocating a minimum of XFS_BUG_KLUDGE bytes temporarily works
+	   around a bug within XFS in which in-inode attribute values are not
+	   checked to see if they will fit in the buffer before they are
+	   copied.  Since no in-core attribute value can be larger than 256
+	   bytes (an 8-bit size field), we allocate that minimum size here to
+	   prevent buffer overrun in both the kernel's and user's buffers.
+	*/
+
+	alloc_size = buflen;
+	if (alloc_size < XFS_BUG_KLUDGE)
+		alloc_size = XFS_BUG_KLUDGE;
+	if (alloc_size > ATTR_MAX_VALUELEN)
+		alloc_size = ATTR_MAX_VALUELEN;
+	value = kmem_zalloc_large(alloc_size, KM_SLEEP);
+
+	/* Get the attribute's value. */
+
+	value_len = alloc_size;		/* in/out parameter */
+
+	error = xfs_attr_get(XFS_I(inode), name.dan_chars, value, &value_len,
+					ATTR_ROOT);
+	DM_EA_XLATE_ERR(error);
+
+	/*
+	 * DMAPI requires an errno of -ENOENT if an attribute does not exist,
+	 * so remap -ENOATTR here.
+	 */
+
+	if (error == -ENOATTR)
+		error = -ENOENT;
+	if (!error && value_len > buflen)
+		error = -E2BIG;
+	if (!error && copy_to_user(bufp, value, value_len))
+		error = -EFAULT;
+	if (!error || error == -E2BIG) {
+		if (put_user(value_len, rlenp))
+			error = -EFAULT;
+	}
+
+	kmem_free(value);
+	return error;
+}
+
+STATIC int
+xfs_dm_get_eventlist(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,
+	u_int 		*nelemp)
+{
+	int		error;
+	xfs_inode_t	*ip = XFS_I(inode);
+
+	if (type == DM_FSYS_OBJ) {
+		error = xfs_dm_fs_get_eventlist(ip->i_mount, right, nelem,
+			eventsetp, nelemp);
+	} else {
+		error = xfs_dm_f_get_eventlist(ip, right, nelem,
+			eventsetp, nelemp);
+	}
+	return error;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_fileattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		mask,		/* not used; always return everything */
+	dm_stat_t	__user *statp)
+{
+	dm_stat_t	stat;
+	xfs_inode_t	*ip = XFS_I(inode);
+	xfs_mount_t	*mp;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	/* Find the mount point. */
+
+	mp = ip->i_mount;
+
+	xfs_ilock(ip, XFS_ILOCK_SHARED);
+	xfs_ip_to_stat(mp, ip->i_ino, ip, &stat);
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+
+	if (copy_to_user( statp, &stat, sizeof(stat)))
+		return -EFAULT;
+	return 0;
+}
+
+
+/* We currently only support a maximum of one managed region per file, and
+   use the DM_EVENT_READ, DM_EVENT_WRITE, and DM_EVENT_TRUNCATE events in
+   the file's dm_eventset_t event mask to implement the DM_REGION_READ,
+   DM_REGION_WRITE, and DM_REGION_TRUNCATE flags for that single region.
+*/
+
+STATIC int
+xfs_dm_get_region(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_region_t	__user *regbufp,
+	u_int		__user *nelemp)
+{
+	dm_eventset_t	evmask;
+	dm_region_t	region;
+	xfs_inode_t	*ip = XFS_I(inode);
+	u_int		elem;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	evmask = ip->i_d.di_dmevmask;	/* read the mask "atomically" */
+
+	/* Get the file's current managed region flags out of the
+	   dm_eventset_t mask and use them to build a managed region that
+	   covers the entire file, i.e. set rg_offset and rg_size to zero.
+	*/
+
+	memset((char *)&region, 0, sizeof(region));
+
+	if (evmask & (1 << DM_EVENT_READ))
+		region.rg_flags |= DM_REGION_READ;
+	if (evmask & (1 << DM_EVENT_WRITE))
+		region.rg_flags |= DM_REGION_WRITE;
+	if (evmask & (1 << DM_EVENT_TRUNCATE))
+		region.rg_flags |= DM_REGION_TRUNCATE;
+
+	elem = (region.rg_flags ? 1 : 0);
+
+	if (copy_to_user( nelemp, &elem, sizeof(elem)))
+		return -EFAULT;
+	if (elem > nelem)
+		return -E2BIG;
+	if (elem && copy_to_user(regbufp, &region, sizeof(region)))
+		return -EFAULT;
+	return 0;
+}
+
+
+STATIC int
+xfs_dm_getall_dmattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	size_t		buflen,
+	void		__user *bufp,
+	size_t		__user *rlenp)
+{
+	attrlist_cursor_kern_t cursor;
+	attrlist_t	*attrlist;
+	dm_attrlist_t	__user *ulist;
+	int		*last_link;
+	int		alignment;
+	int		total_size;
+	int		list_size = 8192;	/* should be big enough */
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	/* Verify that the user gave us a buffer that is 4-byte aligned, lock
+	   it down, and work directly within that buffer.  As a side-effect,
+	   values of buflen < sizeof(int) return -EINVAL.
+	*/
+
+	alignment = sizeof(int) - 1;
+	if ((((uintptr_t)bufp & alignment) != 0) ||
+               !access_ok(VERIFY_WRITE, bufp, buflen)) {
+		return -EFAULT;
+	}
+	buflen &= ~alignment;		/* round down the alignment */
+
+	/* Initialize all the structures and variables for the main loop. */
+
+	memset(&cursor, 0, sizeof(cursor));
+	attrlist = (attrlist_t *)kmem_zalloc_large(list_size, KM_SLEEP);
+	total_size = 0;
+	ulist = (dm_attrlist_t *)bufp;
+	last_link = NULL;
+
+	/* Use vop_attr_list to get the names of DMAPI attributes, and use
+	   vop_attr_get to get their values.  There is a risk here that the
+	   DMAPI attributes could change between the vop_attr_list and
+	   vop_attr_get calls.	If we can detect it, we return -EIO to notify
+	   the user.
+	*/
+
+	do {
+		int	i;
+
+		/* Get a buffer full of attribute names.  If there aren't any
+		   more or if we encounter an error, then finish up.
+		*/
+
+		error = xfs_attr_list(XFS_I(inode), (char *)attrlist, list_size,
+						ATTR_ROOT, &cursor);
+		DM_EA_XLATE_ERR(error);
+
+		if (error || attrlist->al_count == 0)
+			break;
+
+		for (i = 0; i < attrlist->al_count; i++) {
+			attrlist_ent_t	*entry;
+			char		*user_name;
+			int		size_needed;
+			int		value_len;
+
+			/* Skip over all non-DMAPI attributes.	If the
+			   attribute name is too long, we assume it is
+			   non-DMAPI even if it starts with the correct
+			   prefix.
+			*/
+
+			entry = ATTR_ENTRY(attrlist, i);
+			if (strncmp(entry->a_name, dmattr_prefix, DMATTR_PREFIXLEN))
+				continue;
+			user_name = &entry->a_name[DMATTR_PREFIXLEN];
+			if (strlen(user_name) > DM_ATTR_NAME_SIZE)
+				continue;
+
+			/* We have a valid DMAPI attribute to return.  If it
+			   won't fit in the user's buffer, we still need to
+			   keep track of the number of bytes for the user's
+			   next call.
+			*/
+
+
+			size_needed = sizeof(*ulist) + entry->a_valuelen;
+			size_needed = (size_needed + alignment) & ~alignment;
+
+			total_size += size_needed;
+			if (total_size > buflen)
+				continue;
+
+			/* Start by filling in all the fields in the
+			   dm_attrlist_t structure.
+			*/
+
+			strncpy((char *)ulist->al_name.an_chars, user_name,
+				DM_ATTR_NAME_SIZE);
+			ulist->al_data.vd_offset = sizeof(*ulist);
+			ulist->al_data.vd_length = entry->a_valuelen;
+			ulist->_link =	size_needed;
+			last_link = &ulist->_link;
+
+			/* Next read the attribute's value into its correct
+			   location after the dm_attrlist structure.  Any sort
+			   of error indicates that the data is moving under us,
+			   so we return -EIO to let the user know.
+			*/
+
+			value_len = entry->a_valuelen;
+
+			error = xfs_attr_get(XFS_I(inode), entry->a_name,
+						(void *)(ulist + 1), &value_len,
+						ATTR_ROOT);
+			DM_EA_XLATE_ERR(error);
+
+			if (error || value_len != entry->a_valuelen) {
+				error = -EIO;
+				break;
+			}
+
+			ulist = (dm_attrlist_t *)((char *)ulist + ulist->_link);
+		}
+	} while (!error && attrlist->al_more);
+	if (last_link)
+		*last_link = 0;
+
+	if (!error && total_size > buflen)
+		error = -E2BIG;
+	if (!error || error == -E2BIG) {
+		if (put_user(total_size, rlenp))
+			error = -EFAULT;
+	}
+
+	kmem_free(attrlist);
+	return error;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_getall_inherit(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_inherit_t	__user *inheritbufp,
+	u_int		__user *nelemp)
+{
+	return -ENOSYS;
+}
+
+
+/* Initialize location pointer for subsequent dm_get_dirattrs,
+   dm_get_bulkattr, and dm_get_bulkall calls.  The same initialization must
+   work for inode-based routines (dm_get_dirattrs) and filesystem-based
+   routines (dm_get_bulkattr and dm_get_bulkall).  Filesystem-based functions
+   call this routine using the filesystem's root inode.
+*/
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_init_attrloc(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrloc_t	__user *locp)
+{
+	dm_attrloc_t	loc = 0;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return -EFAULT;
+	return 0;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_mkdir_by_handle(
+	struct inode	*inode,
+	dm_right_t	right,
+	void		__user *hanp,
+	size_t		hlen,
+	char		__user *cname)
+{
+	return -ENOSYS;
+}
+
+
+/*
+ * Probe and Punch
+ *
+ * Hole punching alignment is based on the underlying device base
+ * allocation size. Because it is not defined in the DMAPI spec, we
+ * can align how we choose here. Round inwards (offset up and length
+ * down) to the block, extent or page size whichever is bigger. Our
+ * DMAPI implementation rounds the hole geometry strictly inwards. If
+ * this is not possible, return -EINVAL for both for xfs_dm_probe_hole
+ * and xfs_dm_punch_hole which differs from the DMAPI spec.  Note that
+ * length = 0 is special - it means "punch to EOF" and at that point
+ * we treat the punch as remove everything past offset (including
+ * preallocation past EOF).
+ */
+
+STATIC int
+xfs_dm_round_hole(
+	dm_off_t	offset,
+	dm_size_t	length,
+	dm_size_t	align,
+	xfs_fsize_t	filesize,
+	dm_off_t	*roff,
+	dm_size_t	*rlen)
+{
+
+	dm_off_t	off = offset;
+	dm_size_t	len = length;
+
+	/* Try to round offset up to the nearest boundary */
+	*roff = roundup_64(off, align);
+	if ((*roff >= filesize) || (len && (len < align)))
+		return -EINVAL;
+
+	if ((len == 0) || ((off + len) == filesize)) {
+		/* punch to EOF */
+		*rlen = 0;
+	} else {
+		/* Round length down to the nearest boundary. */
+		ASSERT(len >= align);
+		ASSERT(align > (*roff - off));
+		len -= *roff - off;
+		*rlen = len - do_mod(len, align);
+		if (*rlen == 0)
+			return -EINVAL; /* requested length is too small */
+	}
+#ifdef CONFIG_DMAPI_DEBUG
+	printk("xfs_dm_round_hole: off %lld, len %llu, align %llu, "
+	       "filesize %llu, roff %lld, rlen %llu\n",
+	       (long long)offset, (unsigned long long)length,
+	       (unsigned long long)align, filesize, (long long)*roff,
+	       (unsigned long long)*rlen);
+#endif
+	return 0; /* hole geometry successfully rounded */
+}
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_probe_hole(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len,
+	dm_off_t	__user	*roffp,
+	dm_size_t	__user *rlenp)
+{
+	dm_off_t	roff;
+	dm_size_t	rlen;
+	xfs_inode_t	*ip = XFS_I(inode);
+	xfs_mount_t	*mp;
+	uint		lock_flags;
+	xfs_fsize_t	realsize;
+	dm_size_t	align;
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	if ((ip->i_d.di_mode & S_IFMT) != S_IFREG)
+		return -EINVAL;
+
+	mp = ip->i_mount;
+	lock_flags = XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL;
+	xfs_ilock(ip, lock_flags);
+	realsize = XFS_ISIZE(ip);
+	xfs_iunlock(ip, lock_flags);
+
+	if ((off + len) > realsize)
+		return -E2BIG;
+
+	align = 1 << mp->m_sb.sb_blocklog;
+
+	error = xfs_dm_round_hole(off, len, align, realsize, &roff, &rlen);
+	if (error)
+		return error;
+
+	if (copy_to_user( roffp, &roff, sizeof(roff)))
+		return -EFAULT;
+	if (copy_to_user( rlenp, &rlen, sizeof(rlen)))
+		return -EFAULT;
+	return 0;
+}
+
+
+STATIC int
+xfs_dm_punch_hole(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len)
+{
+	int		error = 0;
+	xfs_inode_t	*ip = XFS_I(inode);
+	xfs_mount_t	*mp;
+	dm_size_t	align;
+	xfs_fsize_t	realsize;
+	dm_off_t	roff;
+	dm_size_t	rlen;
+	uint		iolock = XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	/* Make sure there are no leases. */
+	error = break_lease(inode, FMODE_WRITE);
+	if (error)
+		return -EBUSY;
+
+	error = get_write_access(inode);
+	if (error)
+		return -EBUSY;
+
+	mp = ip->i_mount;
+
+	down_rw_sems(inode, DM_SEM_FLAG_WR);
+
+	xfs_ilock(ip, iolock);
+
+	error = xfs_break_layouts(inode, &iolock, false);
+	if (error)
+		goto up_and_out;
+
+	xfs_ilock(ip, XFS_MMAPLOCK_EXCL);
+	iolock |= XFS_MMAPLOCK_EXCL;
+
+	realsize = XFS_ISIZE(ip);
+	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+	iolock &= ~XFS_ILOCK_EXCL;
+
+	align = xfs_get_extsz_hint(ip);
+	if (align == 0)
+		align = 1;
+
+	align <<= mp->m_sb.sb_blocklog;
+
+	if ((off + len) > realsize)
+		goto up_and_out;
+
+	if ((off + len) == realsize)
+		len = 0;
+
+	error = xfs_dm_round_hole(off, len, align, realsize, &roff, &rlen);
+	if (error || (off != roff) || (len != rlen)) {
+		error = -EINVAL;
+		goto up_and_out;
+	}
+
+	/*
+	 * When we are punching to EOF, we have to make sure we punch
+	 * the last partial block that contains EOF. Round up
+	 * the length to make sure we punch the block and not just
+	 * zero it.
+	 */
+	if (!len)
+		len = roundup_64((realsize - off), mp->m_sb.sb_blocksize);
+
+#ifdef CONFIG_DMAPI_DEBUG
+	printk("xfs_dm_punch_hole: off %lld, len %llu, align %llu\n",
+	       (long long)off, (unsigned long long)len,
+	       (unsigned long long)align);
+#endif
+
+
+	xfs_dmapi_mark();
+	error = __xfs_free_file_space(ip, off, len, false, true);
+	xfs_dmapi_unmark();
+
+	/*
+	 * if punching to end of file, kill any blocks past EOF that
+	 * may have been (speculatively) preallocated. No point in
+	 * leaving them around if we are migrating the file....
+	 */
+	if (!error && (len == 0)) {
+		error = xfs_free_eofblocks(mp, ip, false);
+	}
+
+
+	/* Let threads in send_data_event know we punched the file. */
+	ip->i_d.di_dmstate++;
+
+up_and_out:
+	xfs_iunlock(ip, iolock);
+
+	up_rw_sems(inode, DM_SEM_FLAG_WR);
+	put_write_access(inode);
+
+	return error;
+}
+
+
+STATIC int
+xfs_dm_read_invis_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		__user *bufp,
+	int		*rvp)
+{
+	if (right < DM_RIGHT_SHARED)
+		return -EACCES;
+
+	return xfs_dm_rdwr(inode, 0, FMODE_READ, off, len, bufp, rvp);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_release_right(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(dm_handle_t) * 2 + 1];
+
+	if (!xfs_vp_to_hexhandle(inode, type, buffer)) {
+		printf("dm_release_right: old %d type %d handle %s\n",
+			right, type, buffer);
+	} else {
+		printf("dm_release_right: old %d type %d handle "
+			" <INVALID>\n", right, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return 0;
+}
+
+
+STATIC int
+xfs_dm_remove_dmattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	int		setdtime,
+	dm_attrname_t	__user *attrnamep)
+{
+	dm_dkattrname_t name;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return error;
+
+	/* Remove the attribute from the object. */
+
+	error = xfs_attr_remove(XFS_I(inode), name.dan_chars, setdtime ?
+				ATTR_ROOT : (ATTR_ROOT|ATTR_KERNOTIME));
+	DM_EA_XLATE_ERR(error);
+
+	if (error == -ENOATTR)
+		error = -ENOENT;
+	return error;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_request_right(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type,		/* DM_FSYS_OBJ or zero */
+	u_int		flags,
+	dm_right_t	newright)
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(dm_handle_t) * 2 + 1];
+
+	if (!xfs_vp_to_hexhandle(inode, type, buffer)) {
+		printf("dm_request_right: old %d new %d type %d flags 0x%x "
+			"handle %s\n", right, newright, type, flags, buffer);
+	} else {
+		printf("dm_request_right: old %d new %d type %d flags 0x%x "
+			"handle <INVALID>\n", right, newright, type, flags);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return 0;
+}
+
+
+STATIC int
+xfs_dm_set_dmattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrname_t	__user *attrnamep,
+	int		setdtime,
+	size_t		buflen,
+	void		__user *bufp)
+{
+	dm_dkattrname_t name;
+	char		*value;
+	int		alloc_size;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return error;
+	if (buflen > ATTR_MAX_VALUELEN)
+		return -E2BIG;
+
+	/* Copy in the attribute's value and store the <name,value> pair in
+	   the object.	We allocate a buffer of at least one byte even if the
+	   caller specified a buflen of zero.  (A buflen of zero is considered
+	   valid.)
+	*/
+
+	alloc_size = (buflen == 0) ? 1 : buflen;
+	value = kmem_zalloc_large(alloc_size, KM_SLEEP);
+	if (copy_from_user( value, bufp, buflen)) {
+		error = -EFAULT;
+	} else {
+		error = xfs_attr_set(XFS_I(inode), name.dan_chars, value, buflen,
+					setdtime ? ATTR_ROOT :
+					(ATTR_ROOT|ATTR_KERNOTIME));
+		DM_EA_XLATE_ERR(error);
+	}
+	kmem_free(value);
+	return error;
+}
+
+STATIC int
+xfs_dm_set_eventlist(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	int		error;
+	xfs_inode_t	*ip = XFS_I(inode);
+
+	if (type == DM_FSYS_OBJ) {
+		error = xfs_dm_fs_set_eventlist(ip->i_mount, right,
+						eventsetp, maxevent);
+	} else {
+		error = xfs_dm_f_set_eventlist(ip, right, eventsetp, maxevent);
+	}
+	return error;
+}
+
+
+/*
+ *  This turned out not XFS-specific, but leave it here with get_fileattr.
+ */
+
+STATIC int
+xfs_dm_set_fileattr(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		mask,
+	dm_fileattr_t	__user *statp)
+{
+	dm_fileattr_t	stat;
+	struct iattr	iattr;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	if (copy_from_user( &stat, statp, sizeof(stat)))
+		return -EFAULT;
+
+	iattr.ia_valid = 0;
+
+	if (mask & DM_AT_MODE) {
+		iattr.ia_valid |= ATTR_MODE;
+		iattr.ia_mode = stat.fa_mode;
+	}
+	if (mask & DM_AT_UID) {
+		iattr.ia_valid |= ATTR_UID;
+		iattr.ia_uid = make_kuid(&init_user_ns, stat.fa_uid);
+	}
+	if (mask & DM_AT_GID) {
+		iattr.ia_valid |= ATTR_GID;
+		iattr.ia_gid = make_kgid(&init_user_ns, stat.fa_gid);
+	}
+	if (mask & DM_AT_ATIME) {
+		iattr.ia_valid |= ATTR_ATIME;
+		iattr.ia_atime.tv_sec = stat.fa_atime;
+		iattr.ia_atime.tv_nsec = 0;
+                inode->i_atime.tv_sec = stat.fa_atime;
+	}
+	if (mask & DM_AT_MTIME) {
+		iattr.ia_valid |= ATTR_MTIME;
+		iattr.ia_mtime.tv_sec = stat.fa_mtime;
+		iattr.ia_mtime.tv_nsec = 0;
+	}
+	if (mask & DM_AT_CTIME) {
+		iattr.ia_valid |= ATTR_CTIME;
+		iattr.ia_ctime.tv_sec = stat.fa_ctime;
+		iattr.ia_ctime.tv_nsec = 0;
+	}
+
+	/*
+	 * DM_AT_DTIME only takes effect if DM_AT_CTIME is not specified.  We
+	 * overload ctime to also act as dtime, i.e. DM_CONFIG_DTIME_OVERLOAD.
+	 */
+	if ((mask & DM_AT_DTIME) && !(mask & DM_AT_CTIME)) {
+		iattr.ia_valid |= ATTR_CTIME;
+		iattr.ia_ctime.tv_sec = stat.fa_dtime;
+		iattr.ia_ctime.tv_nsec = 0;
+	}
+	if (mask & DM_AT_SIZE) {
+		/* Bits handled by xfs_setattr_size */
+		unsigned int mask = ATTR_MODE | ATTR_ATIME | ATTR_CTIME |
+				    ATTR_MTIME | ATTR_SIZE;
+		struct iattr sattr = iattr;
+
+		/*
+		 * With the split in xfs_setattr to size and nonsize
+		 * variants, we may need to call both to complete this
+		 * operation.  Since we need to call xfs_setxattr_size,
+		 * let it do as much as it can before deciding whether
+		 * to call xfs_setxattr_nonsize.
+		 */
+		iattr.ia_valid &= ~mask;
+		sattr.ia_valid &= mask;
+		sattr.ia_valid |= ATTR_SIZE;
+		sattr.ia_size = stat.fa_size;
+
+		xfs_dmapi_mark();
+		error = xfs_setattr_size(XFS_I(inode), &sattr);
+		xfs_dmapi_unmark();
+		if (error)
+			return error;
+	}
+
+	if (!iattr.ia_valid)
+		return 0;
+
+	xfs_dmapi_mark();
+	error = xfs_setattr_nonsize(XFS_I(inode), &iattr, 0);
+	xfs_dmapi_unmark();
+	return error;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_set_inherit(
+	struct inode	*inode,
+	dm_right_t	right,
+	dm_attrname_t	__user *attrnamep,
+	mode_t		mode)
+{
+	return -ENOSYS;
+}
+
+
+STATIC int
+xfs_dm_set_region(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_region_t	__user *regbufp,
+	dm_boolean_t	__user *exactflagp)
+{
+	xfs_inode_t	*ip = XFS_I(inode);
+	xfs_trans_t	*tp;
+	xfs_mount_t	*mp;
+	dm_region_t	region;
+	dm_eventset_t	new_mask;
+	dm_eventset_t	mr_mask;
+	struct xfs_trans_res	tres;
+	int		error;
+	u_int		exactflag;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	/* If the caller gave us more than one dm_region_t structure, complain.
+	   (He has to call dm_get_config() to find out what our limit is.)
+	*/
+
+	if (nelem > 1)
+		return -E2BIG;
+
+	/* If the user provided a dm_region_t structure, then copy it in,
+	   validate it, and convert its flags to the corresponding bits in a
+	   dm_set_eventlist() event mask.  A call with zero regions is
+	   equivalent to clearing all region flags.
+	*/
+
+	new_mask = 0;
+	if (nelem == 1) {
+		if (copy_from_user( &region, regbufp, sizeof(region)))
+			return -EFAULT;
+
+		if (region.rg_flags & ~(DM_REGION_READ|DM_REGION_WRITE|DM_REGION_TRUNCATE))
+			return -EINVAL;
+		if (region.rg_flags & DM_REGION_READ)
+			new_mask |= 1 << DM_EVENT_READ;
+		if (region.rg_flags & DM_REGION_WRITE)
+			new_mask |= 1 << DM_EVENT_WRITE;
+		if (region.rg_flags & DM_REGION_TRUNCATE)
+			new_mask |= 1 << DM_EVENT_TRUNCATE;
+	}
+	mr_mask = (1 << DM_EVENT_READ) | (1 << DM_EVENT_WRITE) | (1 << DM_EVENT_TRUNCATE);
+
+	/* Get the file's existing event mask, clear the old managed region
+	   bits, add in the new ones, and update the file's mask.
+	*/
+
+	if (new_mask & prohibited_mr_events(inode->i_mapping)) {
+		/* If the change is simply to remove the READ
+		 * bit, then that's always okay.  Otherwise, it's busy.
+		 */
+		dm_eventset_t m1;
+		m1 = ip->i_d.di_dmevmask & ((1 << DM_EVENT_WRITE) | (1 << DM_EVENT_TRUNCATE));
+		if (m1 != new_mask) {
+			return -EBUSY;
+		}
+	}
+
+	mp = ip->i_mount;
+	tres.tr_logres = M_RES(mp)->tr_ichange.tr_logres;
+	tres.tr_logcount = 0;
+	tres.tr_logflags = 0;
+	tp = xfs_trans_alloc(mp, XFS_TRANS_SET_DMATTRS);
+	error = xfs_trans_reserve(tp, &tres, 0, 0);
+	if (error) {
+		xfs_trans_cancel(tp);
+		return error;
+	}
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+
+	ip->i_d.di_dmevmask = (ip->i_d.di_dmevmask & ~mr_mask) | new_mask;
+
+	xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+//	igrab(inode);
+	xfs_trans_commit(tp);
+
+	/* Return the proper value for *exactflagp depending upon whether or not
+	   we "changed" the user's managed region.  In other words, if the user
+	   specified a non-zero value for either rg_offset or rg_size, we
+	   round each of those values back to zero.
+	*/
+
+	if (nelem && (region.rg_offset || region.rg_size)) {
+		exactflag = DM_FALSE;	/* user region was changed */
+	} else {
+		exactflag = DM_TRUE;	/* user region was unchanged */
+	}
+	if (copy_to_user( exactflagp, &exactflag, sizeof(exactflag)))
+		return -EFAULT;
+	return 0;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_symlink_by_handle(
+	struct inode	*inode,
+	dm_right_t	right,
+	void __user	*hanp,
+	size_t		hlen,
+	char		__user *cname,
+	char		__user *path)
+{
+	return -ENOSYS;
+}
+
+
+STATIC int
+xfs_dm_sync_by_handle(
+	struct inode	*inode,
+	dm_right_t	right)
+{
+	int		err;
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	err = xfs_fsync(XFS_I(inode), 0);
+
+	return err;
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_upgrade_right(
+	struct inode	*inode,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(dm_handle_t) * 2 + 1];
+
+	if (!xfs_vp_to_hexhandle(inode, type, buffer)) {
+		printf("dm_upgrade_right: old %d new %d type %d handle %s\n",
+			right, DM_RIGHT_EXCL, type, buffer);
+	} else {
+		printf("dm_upgrade_right: old %d new %d type %d handle "
+			"<INVALID>\n", right, DM_RIGHT_EXCL, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return 0;
+}
+
+
+STATIC int
+xfs_dm_write_invis_rvp(
+	struct inode	*inode,
+	dm_right_t	right,
+	int		flags,
+	dm_off_t	off,
+	dm_size_t	len,
+	void __user	*bufp,
+	int		*rvp)
+{
+	int		fflag = 0;
+
+
+	if (right < DM_RIGHT_EXCL)
+		return -EACCES;
+
+	if (flags & DM_WRITE_SYNC)
+		fflag |= O_SYNC;
+	return xfs_dm_rdwr(inode, fflag, FMODE_WRITE, off, len, bufp, rvp);
+}
+
+
+STATIC void
+xfs_dm_obj_ref_hold(
+	struct inode	*inode)
+{
+	igrab(inode);
+}
+
+
+static fsys_function_vector_t xfs_fsys_vector[DM_FSYS_MAX] = {
+	{
+	  .func_no = DM_FSYS_CLEAR_INHERIT,
+	  .u_fc.clear_inherit = xfs_dm_clear_inherit,
+	},
+	{
+	  .func_no = DM_FSYS_CREATE_BY_HANDLE,
+	  .u_fc.create_by_handle = xfs_dm_create_by_handle,
+	},
+	{
+	  .func_no = DM_FSYS_DOWNGRADE_RIGHT,
+	  .u_fc.downgrade_right = xfs_dm_downgrade_right,
+	},
+	{
+	  .func_no = DM_FSYS_GET_ALLOCINFO_RVP,
+	  .u_fc.get_allocinfo_rvp = xfs_dm_get_allocinfo_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_GET_BULKALL_RVP,
+	  .u_fc.get_bulkall_rvp = xfs_dm_get_bulkall_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_GET_BULKATTR_RVP,
+	  .u_fc.get_bulkattr_rvp = xfs_dm_get_bulkattr_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_GET_CONFIG,
+	  .u_fc.get_config = xfs_dm_get_config,
+	},
+	{
+	  .func_no = DM_FSYS_GET_CONFIG_EVENTS,
+	  .u_fc.get_config_events = xfs_dm_get_config_events,
+	},
+	{
+	  .func_no = DM_FSYS_GET_DESTROY_DMATTR,
+	  .u_fc.get_destroy_dmattr = xfs_dm_get_destroy_dmattr,
+	},
+	{
+	  .func_no = DM_FSYS_GET_DIOINFO,
+	  .u_fc.get_dioinfo = xfs_dm_get_dioinfo,
+	},
+	{
+	  .func_no = DM_FSYS_GET_DIRATTRS_RVP,
+	  .u_fc.get_dirattrs_rvp = xfs_dm_get_dirattrs_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_GET_DMATTR,
+	  .u_fc.get_dmattr = xfs_dm_get_dmattr,
+	},
+	{
+	  .func_no = DM_FSYS_GET_EVENTLIST,
+	  .u_fc.get_eventlist = xfs_dm_get_eventlist,
+	},
+	{
+	  .func_no = DM_FSYS_GET_FILEATTR,
+	  .u_fc.get_fileattr = xfs_dm_get_fileattr,
+	},
+	{
+	  .func_no = DM_FSYS_GET_REGION,
+	  .u_fc.get_region = xfs_dm_get_region,
+	},
+	{
+	  .func_no = DM_FSYS_GETALL_DMATTR,
+	  .u_fc.getall_dmattr = xfs_dm_getall_dmattr,
+	},
+	{
+	  .func_no = DM_FSYS_GETALL_INHERIT,
+	  .u_fc.getall_inherit = xfs_dm_getall_inherit,
+	},
+	{
+	  .func_no = DM_FSYS_INIT_ATTRLOC,
+	  .u_fc.init_attrloc = xfs_dm_init_attrloc,
+	},
+	{
+	  .func_no = DM_FSYS_MKDIR_BY_HANDLE,
+	  .u_fc.mkdir_by_handle = xfs_dm_mkdir_by_handle,
+	},
+	{
+	  .func_no = DM_FSYS_PROBE_HOLE,
+	  .u_fc.probe_hole = xfs_dm_probe_hole,
+	},
+	{
+	  .func_no = DM_FSYS_PUNCH_HOLE,
+	  .u_fc.punch_hole = xfs_dm_punch_hole,
+	},
+	{
+	  .func_no = DM_FSYS_READ_INVIS_RVP,
+	  .u_fc.read_invis_rvp = xfs_dm_read_invis_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_RELEASE_RIGHT,
+	  .u_fc.release_right = xfs_dm_release_right,
+	},
+	{
+	  .func_no = DM_FSYS_REMOVE_DMATTR,
+	  .u_fc.remove_dmattr = xfs_dm_remove_dmattr,
+	},
+	{
+	  .func_no = DM_FSYS_REQUEST_RIGHT,
+	  .u_fc.request_right = xfs_dm_request_right,
+	},
+	{
+	  .func_no = DM_FSYS_SET_DMATTR,
+	  .u_fc.set_dmattr = xfs_dm_set_dmattr,
+	},
+	{
+	  .func_no = DM_FSYS_SET_EVENTLIST,
+	  .u_fc.set_eventlist = xfs_dm_set_eventlist,
+	},
+	{
+	  .func_no = DM_FSYS_SET_FILEATTR,
+	  .u_fc.set_fileattr = xfs_dm_set_fileattr,
+	},
+	{
+	  .func_no = DM_FSYS_SET_INHERIT,
+	  .u_fc.set_inherit = xfs_dm_set_inherit,
+	},
+	{
+	  .func_no = DM_FSYS_SET_REGION,
+	  .u_fc.set_region = xfs_dm_set_region,
+	},
+	{
+	  .func_no = DM_FSYS_SYMLINK_BY_HANDLE,
+	  .u_fc.symlink_by_handle = xfs_dm_symlink_by_handle,
+	},
+	{
+	  .func_no = DM_FSYS_SYNC_BY_HANDLE,
+	  .u_fc.sync_by_handle = xfs_dm_sync_by_handle,
+	},
+	{
+	  .func_no = DM_FSYS_UPGRADE_RIGHT,
+	  .u_fc.upgrade_right = xfs_dm_upgrade_right,
+	},
+	{
+	  .func_no = DM_FSYS_WRITE_INVIS_RVP,
+	  .u_fc.write_invis_rvp = xfs_dm_write_invis_rvp,
+	},
+	{
+	  .func_no = DM_FSYS_OBJ_REF_HOLD,
+	  .u_fc.obj_ref_hold = xfs_dm_obj_ref_hold,
+	},
+};
+
+STATIC int
+xfs_dm_get_dmapiops(
+	struct super_block	*sb,
+	void			*addr)
+{
+	dm_fcntl_vector_t	*vecrq = (dm_fcntl_vector_t *)addr;
+
+	vecrq->code_level = DM_CLVL_XOPEN;
+	vecrq->count = ARRAY_SIZE(xfs_fsys_vector);
+	vecrq->vecp = xfs_fsys_vector;
+
+	return 0 ;
+}
+
+
+/*	xfs_dm_send_mmap_event - send events needed for memory mapping a file.
+ *
+ *	This is a workaround called for files that are about to be
+ *	mapped.	 DMAPI events are not being generated at a low enough level
+ *	in the kernel for page reads/writes to generate the correct events.
+ *	So for memory-mapped files we generate read  or write events for the
+ *	whole byte range being mapped.	If the mmap call can never cause a
+ *	write to the file, then only a read event is sent.
+ *
+ *	Code elsewhere prevents adding managed regions to a file while it
+ *	is still mapped.
+ */
+
+STATIC int
+xfs_dm_send_mmap_event(
+	struct vm_area_struct *vma,
+	unsigned int	wantflag)
+{
+	xfs_inode_t	*ip;
+	int		error = 0;
+	dm_eventtype_t	max_event = DM_EVENT_READ;
+	xfs_fsize_t	filesize;
+	xfs_off_t	length, end_of_area, evsize, offset;
+	int		iolock;
+
+	if (!vma->vm_file)
+		return 0;
+
+	ip = XFS_I(file_inode(vma->vm_file));
+
+	if (!S_ISREG(file_inode(vma->vm_file)->i_mode) ||
+	    !(ip->i_mount->m_flags & XFS_MOUNT_DMAPI))
+		return 0;
+
+	/* If they specifically asked for 'read', then give it to them.
+	 * Otherwise, see if it's possible to give them 'write'.
+	 */
+	if( wantflag & VM_READ ){
+		max_event = DM_EVENT_READ;
+	}
+	else if( ! (vma->vm_flags & VM_DENYWRITE) ) {
+		if((wantflag & VM_WRITE) || (vma->vm_flags & VM_WRITE))
+			max_event = DM_EVENT_WRITE;
+	}
+
+	if( (wantflag & VM_WRITE) && (max_event != DM_EVENT_WRITE) ){
+		return -EACCES;
+	}
+
+	/* Figure out how much of the file is being requested by the user. */
+	offset = 0; /* beginning of file, for now */
+	length = 0; /* whole file, for now */
+
+	filesize = i_size_read(file_inode(vma->vm_file));
+
+	/* Set first byte number beyond the map area. */
+
+	if (length) {
+		end_of_area = offset + length;
+		if (end_of_area > filesize)
+			end_of_area = filesize;
+	} else {
+		end_of_area = filesize;
+	}
+
+	/* Set the real amount being mapped. */
+	evsize = end_of_area - offset;
+	if (evsize < 0)
+		evsize = 0;
+
+	if (max_event == DM_EVENT_READ)
+		iolock = XFS_IOLOCK_SHARED;
+	else
+		iolock = XFS_IOLOCK_EXCL;
+
+	xfs_ilock(ip, iolock);
+	/* If write possible, try a DMAPI write event */
+	if (max_event == DM_EVENT_WRITE && DM_EVENT_ENABLED(ip, max_event)) {
+		error = xfs_dm_send_data_event(max_event, ip, offset,
+					       evsize, 0, &iolock);
+		goto out_unlock;
+	}
+
+	/* Try a read event if max_event was != DM_EVENT_WRITE or if it
+	 * was DM_EVENT_WRITE but the WRITE event was not enabled.
+	 */
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_READ)) {
+		error = xfs_dm_send_data_event(DM_EVENT_READ, ip, offset,
+					       evsize, 0, &iolock);
+	}
+out_unlock:
+	xfs_iunlock(ip, iolock);
+	return error;
+}
+
+
+STATIC int
+xfs_dm_send_destroy_event(
+	xfs_inode_t	*ip,
+	dm_right_t	vp_right)	/* always DM_RIGHT_NULL */
+{
+	return dm_send_destroy_event(VFS_I(ip), vp_right);
+}
+
+
+STATIC int
+xfs_dm_send_namesp_event(
+	dm_eventtype_t	event,
+	struct xfs_mount *mp,
+	struct xfs_inode *ip1,
+	dm_right_t	vp1_right,
+	struct xfs_inode *ip2,
+	dm_right_t	vp2_right,
+	const unsigned char	*name1,
+	const unsigned char	*name2,
+	mode_t		mode,
+	int		retcode,
+	int		flags)
+{
+	return dm_send_namesp_event(event, mp ? mp->m_super : NULL,
+				     VFS_I(ip1), vp1_right,
+				     ip2 ? VFS_I(ip2) : NULL, vp2_right,
+				     name1, name2,
+				     mode, retcode, flags);
+}
+
+STATIC int
+xfs_dm_send_mount_event(
+	struct xfs_mount	*mp,
+	dm_right_t		root_right,
+	char			*mtpt,
+	char			*fsname)
+{
+	return dm_send_mount_event(mp->m_super, root_right,
+			NULL, DM_RIGHT_NULL,
+			mp->m_rootip ? VFS_I(mp->m_rootip) : NULL,
+			DM_RIGHT_NULL, mtpt, fsname);
+}
+
+STATIC void
+xfs_dm_send_unmount_event(
+	struct xfs_mount *mp,
+	xfs_inode_t	*ip,		/* NULL if unmount successful */
+	dm_right_t	vfsp_right,
+	mode_t		mode,
+	int		retcode,	/* errno, if unmount failed */
+	int		flags)
+{
+	dm_send_unmount_event(mp->m_super, ip ? VFS_I(ip) : NULL,
+			      vfsp_right, mode, retcode, flags);
+}
+
+
+/*
+ * Data migration operations accessed by the rest of XFS.
+ * When DMAPI support is configured in, this vector is used.
+ */
+
+xfs_dmops_t	xfs_dmcore_xfs = {
+	.xfs_send_data		= xfs_dm_send_data_event,
+	.xfs_send_mmap		= xfs_dm_send_mmap_event,
+	.xfs_send_destroy	= xfs_dm_send_destroy_event,
+	.xfs_send_namesp	= xfs_dm_send_namesp_event,
+	.xfs_send_mount		= xfs_dm_send_mount_event,
+	.xfs_send_unmount	= xfs_dm_send_unmount_event,
+};
+EXPORT_SYMBOL(xfs_dmcore_xfs);
+
+STATIC int
+xfs_dm_fh_to_inode(
+	struct super_block	*sb,
+	struct inode		**inode,
+	dm_fid_t		*dmfid)
+{
+	xfs_mount_t		*mp = XFS_M(sb);
+	xfs_inode_t		*ip;
+	xfs_ino_t		ino;
+	unsigned int		igen;
+	int			error;
+
+	*inode = NULL;
+
+	if (!dmfid->dm_fid_len) {
+		/* filesystem handle */
+		*inode = igrab(VFS_I(mp->m_rootip));
+		if (!*inode)
+			return -ENOENT;
+		return 0;
+	}
+
+	if (dmfid->dm_fid_len != sizeof(*dmfid) - sizeof(dmfid->dm_fid_len))
+		return -EINVAL;
+
+	ino  = dmfid->dm_fid_ino;
+	igen = dmfid->dm_fid_gen;
+
+	/* fail requests for ino 0 gracefully. */
+	if (ino == 0)
+		return -ESTALE;
+
+	error = xfs_iget(mp, NULL, ino, 0, XFS_ILOCK_SHARED, &ip);
+	if (error)
+		return error;
+	if (!ip)
+		return -EIO;
+
+	if (!ip->i_d.di_mode || ip->i_d.di_gen != igen) {
+		xfs_iunlock(ip, XFS_ILOCK_SHARED);
+		IRELE(ip);
+		return -ENOENT;
+	}
+
+	*inode = VFS_I(ip);
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+	return 0;
+}
+
+STATIC int
+xfs_dm_inode_to_fh(
+	struct inode		*inode,
+	dm_fid_t		*dmfid,
+	dm_fsid_t		*dmfsid)
+{
+	xfs_inode_t		*ip = XFS_I(inode);
+
+	if (ip->i_mount->m_fixedfsid == NULL)
+		return -EINVAL;
+
+	dmfid->dm_fid_len = sizeof(dm_fid_t) - sizeof(dmfid->dm_fid_len);
+	dmfid->dm_fid_pad = 0;
+	/*
+	 * use memcpy because the inode is a long long and there's no
+	 * assurance that dmfid->dm_fid_ino is properly aligned.
+	 */
+	memcpy(&dmfid->dm_fid_ino, &ip->i_ino, sizeof(dmfid->dm_fid_ino));
+	dmfid->dm_fid_gen = ip->i_d.di_gen;
+
+	memcpy(dmfsid, ip->i_mount->m_fixedfsid, sizeof(*dmfsid));
+	return 0;
+}
+
+STATIC void
+xfs_dm_get_fsid(
+	struct super_block	*sb,
+	dm_fsid_t		*fsid)
+{
+	memcpy(fsid, XFS_M(sb)->m_fixedfsid, sizeof(*fsid));
+}
+
+/*
+ * Filesystem operations accessed by the DMAPI core.
+ */
+static struct filesystem_dmapi_operations xfs_dmapiops = {
+	.get_fsys_vector	= xfs_dm_get_dmapiops,
+	.fh_to_inode		= xfs_dm_fh_to_inode,
+	.inode_to_fh		= xfs_dm_inode_to_fh,
+	.get_fsid		= xfs_dm_get_fsid,
+};
+
+static int __init
+xfs_dm_init(void)
+{
+	printk(KERN_INFO "SGI XFS Data Management API subsystem\n");
+
+	dmapi_register(&xfs_fs_type, &xfs_dmapiops);
+	return 0;
+}
+
+static void __exit
+xfs_dm_exit(void)
+{
+	dmapi_unregister(&xfs_fs_type);
+}
+
+MODULE_AUTHOR("SGI");
+MODULE_DESCRIPTION("SGI XFS dmapi subsystem");
+MODULE_LICENSE("GPL");
+
+module_init(xfs_dm_init);
+module_exit(xfs_dm_exit);
--- /dev/null
+++ b/fs/xfs/dmapi/xfs_dm.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright (c) 2006 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef __XFS_DM_H__
+#define __XFS_DM_H__
+
+extern struct file_system_type xfs_fs_type;
+
+static inline void xfs_dmapi_mark(void)
+{
+	current->journal_info = ERR_PTR(-EBUSY);
+}
+
+static inline void xfs_dmapi_unmark(void)
+{
+	current->journal_info = NULL;
+}
+
+static inline bool xfs_dmapi_marked(void)
+{
+	return current->journal_info == ERR_PTR(-EBUSY);
+}
+
+#endif /* __XFS_DM_H__ */
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@ -45,6 +45,7 @@
 #include "xfs_symlink.h"
 #include "xfs_attr_leaf.h"
 #include "xfs_filestream.h"
+#include "xfs_dmapi.h"
 
 
 kmem_zone_t		*xfs_bmap_free_item_zone;
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@ -40,6 +40,8 @@
 #include "xfs_trace.h"
 #include "xfs_icache.h"
 #include "xfs_log.h"
+#include "xfs_dmapi.h"
+#include "dmapi/xfs_dm.h"
 
 /* Kernel only BMAP related definitions and functions */
 
@@ -559,6 +561,28 @@ xfs_getbmap(
 		prealloced = 0;
 		fixlen = 1LL << 32;
 	} else {
+		/*
+		 * If the BMV_IF_NO_DMAPI_READ interface bit specified, do
+		 * not generate a DMAPI read event.  Otherwise, if the
+		 * DM_EVENT_READ bit is set for the file, generate a read
+		 * event in order that the DMAPI application may do its thing
+		 * before we return the extents.  Usually this means restoring
+		 * user file data to regions of the file that look like holes.
+		 *
+		 * The "old behavior" (from XFS_IOC_GETBMAP) is to not specify
+		 * BMV_IF_NO_DMAPI_READ so that read events are generated.
+		 * If this were not true, callers of ioctl(XFS_IOC_GETBMAP)
+		 * could misinterpret holes in a DMAPI file as true holes,
+		 * when in fact they may represent offline user data.
+		 */
+		if (DM_EVENT_ENABLED(ip, DM_EVENT_READ) &&
+		    !(iflags & BMV_IF_NO_DMAPI_READ)) {
+			error = XFS_SEND_DATA(mp, DM_EVENT_READ, ip,
+					      0, 0, 0, NULL);
+			if (error)
+				return error;
+		}
+
 		if (ip->i_d.di_format != XFS_DINODE_FMT_EXTENTS &&
 		    ip->i_d.di_format != XFS_DINODE_FMT_BTREE &&
 		    ip->i_d.di_format != XFS_DINODE_FMT_LOCAL)
@@ -993,9 +1017,25 @@ xfs_alloc_file_space(
 	startoffset_fsb	= XFS_B_TO_FSBT(mp, offset);
 	allocatesize_fsb = XFS_B_TO_FSB(mp, count);
 
+	/*	Generate a DMAPI event if needed.	*/
+	if (alloc_type != 0 && offset < XFS_ISIZE(ip) &&
+			!xfs_dmapi_marked() &&
+			DM_EVENT_ENABLED(ip, DM_EVENT_WRITE)) {
+		xfs_off_t           end_dmi_offset;
+
+		end_dmi_offset = offset+len;
+		if (end_dmi_offset > XFS_ISIZE(ip))
+			end_dmi_offset = XFS_ISIZE(ip);
+		error = XFS_SEND_DATA(mp, DM_EVENT_WRITE, ip, offset,
+				      end_dmi_offset - offset, 0, NULL);
+		if (error)
+			return error;
+	}
+
 	/*
 	 * Allocate file space until done or until there is an error
 	 */
+retry:
 	while (allocatesize_fsb && !error) {
 		xfs_fileoff_t	s, e;
 
@@ -1092,6 +1132,17 @@ xfs_alloc_file_space(
 		startoffset_fsb += allocated_fsb;
 		allocatesize_fsb -= allocated_fsb;
 	}
+dmapi_enospc_check:
+	if (error == ENOSPC && !xfs_dmapi_marked() &&
+	    DM_EVENT_ENABLED(ip, DM_EVENT_NOSPACE)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_NOSPACE,
+				ip, DM_RIGHT_NULL,
+				ip, DM_RIGHT_NULL,
+				NULL, NULL, 0, 0, 0); /* Delay flag intentionally unused */
+		if (error == 0)
+			goto retry;	/* Maybe DMAPI app. has made space */
+		/* else fall through with error from XFS_SEND_DATA */
+	}
 
 	return error;
 
@@ -1102,7 +1153,7 @@ error0:	/* Cancel bmap, unlock inode, un
 error1:	/* Just cancel transaction */
 	xfs_trans_cancel(tp);
 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
-	return error;
+	goto dmapi_enospc_check;
 }
 
 /*
@@ -1201,13 +1252,16 @@ xfs_zero_remaining_bytes(
 }
 
 int
-xfs_free_file_space(
+__xfs_free_file_space(
 	struct xfs_inode	*ip,
 	xfs_off_t		offset,
-	xfs_off_t		len)
+	xfs_off_t		len,
+	bool			dmapi_nonblock,
+	bool			rounding_done)
 {
 	int			committed;
 	int			done;
+	xfs_off_t		end_dmi_offset;
 	xfs_fileoff_t		endoffset_fsb;
 	int			error;
 	xfs_fsblock_t		firstfsb;
@@ -1237,14 +1291,32 @@ xfs_free_file_space(
 		return error;
 	rt = XFS_IS_REALTIME_INODE(ip);
 	startoffset_fsb	= XFS_B_TO_FSB(mp, offset);
-	endoffset_fsb = XFS_B_TO_FSBT(mp, offset + len);
+	end_dmi_offset = offset + len;
+	endoffset_fsb = XFS_B_TO_FSBT(mp, end_dmi_offset);
+
+	if (offset < XFS_ISIZE(ip) && !xfs_dmapi_marked() &&
+	    DM_EVENT_ENABLED(ip, DM_EVENT_WRITE)) {
+		if (end_dmi_offset > XFS_ISIZE(ip))
+			end_dmi_offset = XFS_ISIZE(ip);
+		error = XFS_SEND_DATA(mp, DM_EVENT_WRITE, ip,
+				offset, end_dmi_offset - offset,
+				dmapi_nonblock ? DM_FLAGS_NDELAY : 0, NULL);
+		if (error)
+			return error;
+	}
 
 	/* wait for the completion of any pending DIOs */
 	inode_dio_wait(VFS_I(ip));
 
-	rounding = max_t(xfs_off_t, 1 << mp->m_sb.sb_blocklog, PAGE_CACHE_SIZE);
-	ioffset = round_down(offset, rounding);
-	iendoffset = round_up(offset + len, rounding) - 1;
+	if (!rounding_done) {
+		rounding = max_t(xfs_off_t, 1 << mp->m_sb.sb_blocklog, PAGE_CACHE_SIZE);
+		ioffset = round_down(offset, rounding);
+		iendoffset = round_up(offset + len, rounding) - 1;
+	} else {
+		ioffset = offset;
+		iendoffset = offset + len - 1;
+	}
+
 	error = filemap_write_and_wait_range(VFS_I(ip)->i_mapping, ioffset,
 					     iendoffset);
 	if (error)
@@ -1373,6 +1445,15 @@ xfs_free_file_space(
 	goto out;
 }
 
+int
+xfs_free_file_space(
+	struct xfs_inode	*ip,
+	xfs_off_t		offset,
+	xfs_off_t		len)
+{
+	return __xfs_free_file_space(ip, offset, len, false, false);
+}
+
 /*
  * Preallocate and zero a range of a file. This mechanism has the allocation
  * semantics of fallocate and in addition converts data in the range to zeroes.
--- a/fs/xfs/xfs_bmap_util.h
+++ b/fs/xfs/xfs_bmap_util.h
@@ -57,6 +57,9 @@ int	xfs_bmap_last_extent(struct xfs_tran
 /* preallocation and hole punch interface */
 int	xfs_alloc_file_space(struct xfs_inode *ip, xfs_off_t offset,
 			     xfs_off_t len, int alloc_type);
+int	__xfs_free_file_space(struct xfs_inode *ip, xfs_off_t offset,
+			    xfs_off_t len, bool dmapi_nonblock,
+			    bool keep_rounding);
 int	xfs_free_file_space(struct xfs_inode *ip, xfs_off_t offset,
 			    xfs_off_t len);
 int	xfs_zero_file_space(struct xfs_inode *ip, xfs_off_t offset,
--- /dev/null
+++ b/fs/xfs/xfs_dmapi.h
@@ -0,0 +1,234 @@
+/*
+ * Copyright (c) 2000-2005 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef __XFS_DMAPI_H__
+#define __XFS_DMAPI_H__
+
+/*	Values used to define the on-disk version of dm_attrname_t. All
+ *	on-disk attribute names start with the 8-byte string "SGI_DMI_".
+ *
+ *      In the on-disk inode, DMAPI attribute names consist of the user-provided
+ *      name with the DMATTR_PREFIXSTRING pre-pended.  This string must NEVER be
+ *      changed.
+ */
+
+#define DMATTR_PREFIXLEN	8
+#define DMATTR_PREFIXSTRING	"SGI_DMI_"
+
+typedef enum {
+	DM_EVENT_INVALID	= -1,
+	DM_EVENT_CANCEL		= 0,		/* not supported */
+	DM_EVENT_MOUNT		= 1,
+	DM_EVENT_PREUNMOUNT	= 2,
+	DM_EVENT_UNMOUNT	= 3,
+	DM_EVENT_DEBUT		= 4,		/* not supported */
+	DM_EVENT_CREATE		= 5,
+	DM_EVENT_CLOSE		= 6,		/* not supported */
+	DM_EVENT_POSTCREATE	= 7,
+	DM_EVENT_REMOVE		= 8,
+	DM_EVENT_POSTREMOVE	= 9,
+	DM_EVENT_RENAME		= 10,
+	DM_EVENT_POSTRENAME	= 11,
+	DM_EVENT_LINK		= 12,
+	DM_EVENT_POSTLINK	= 13,
+	DM_EVENT_SYMLINK	= 14,
+	DM_EVENT_POSTSYMLINK	= 15,
+	DM_EVENT_READ		= 16,
+	DM_EVENT_WRITE		= 17,
+	DM_EVENT_TRUNCATE	= 18,
+	DM_EVENT_ATTRIBUTE	= 19,
+	DM_EVENT_DESTROY	= 20,
+	DM_EVENT_NOSPACE	= 21,
+	DM_EVENT_USER		= 22,
+	DM_EVENT_MAX		= 23
+} dm_eventtype_t;
+#define HAVE_DM_EVENTTYPE_T
+
+typedef enum {
+	DM_RIGHT_NULL,
+	DM_RIGHT_SHARED,
+	DM_RIGHT_EXCL
+} dm_right_t;
+#define HAVE_DM_RIGHT_T
+
+/* Defines for determining if an event message should be sent. */
+#ifdef HAVE_DMAPI
+#define	DM_EVENT_ENABLED(ip, event) ( \
+	unlikely ((ip)->i_mount->m_flags & XFS_MOUNT_DMAPI) && \
+		( ((ip)->i_d.di_dmevmask & (1 << event)) || \
+		  ((ip)->i_mount->m_dmevmask & (1 << event)) ) \
+	)
+#else
+#define DM_EVENT_ENABLED(ip, event)	(0)
+#endif
+
+#define DM_XFS_VALID_FS_EVENTS		( \
+	(1 << DM_EVENT_PREUNMOUNT)	| \
+	(1 << DM_EVENT_UNMOUNT)		| \
+	(1 << DM_EVENT_NOSPACE)		| \
+	(1 << DM_EVENT_DEBUT)		| \
+	(1 << DM_EVENT_CREATE)		| \
+	(1 << DM_EVENT_POSTCREATE)	| \
+	(1 << DM_EVENT_REMOVE)		| \
+	(1 << DM_EVENT_POSTREMOVE)	| \
+	(1 << DM_EVENT_RENAME)		| \
+	(1 << DM_EVENT_POSTRENAME)	| \
+	(1 << DM_EVENT_LINK)		| \
+	(1 << DM_EVENT_POSTLINK)	| \
+	(1 << DM_EVENT_SYMLINK)		| \
+	(1 << DM_EVENT_POSTSYMLINK)	| \
+	(1 << DM_EVENT_ATTRIBUTE)	| \
+	(1 << DM_EVENT_DESTROY)		)
+
+/* Events valid in dm_set_eventlist() when called with a file handle for
+   a regular file or a symlink.  These events are persistent.
+*/
+
+#define	DM_XFS_VALID_FILE_EVENTS	( \
+	(1 << DM_EVENT_ATTRIBUTE)	| \
+	(1 << DM_EVENT_DESTROY)		)
+
+/* Events valid in dm_set_eventlist() when called with a file handle for
+   a directory.  These events are persistent.
+*/
+
+#define	DM_XFS_VALID_DIRECTORY_EVENTS	( \
+	(1 << DM_EVENT_CREATE)		| \
+	(1 << DM_EVENT_POSTCREATE)	| \
+	(1 << DM_EVENT_REMOVE)		| \
+	(1 << DM_EVENT_POSTREMOVE)	| \
+	(1 << DM_EVENT_RENAME)		| \
+	(1 << DM_EVENT_POSTRENAME)	| \
+	(1 << DM_EVENT_LINK)		| \
+	(1 << DM_EVENT_POSTLINK)	| \
+	(1 << DM_EVENT_SYMLINK)		| \
+	(1 << DM_EVENT_POSTSYMLINK)	| \
+	(1 << DM_EVENT_ATTRIBUTE)	| \
+	(1 << DM_EVENT_DESTROY)		)
+
+/* Events supported by the XFS filesystem. */
+#define	DM_XFS_SUPPORTED_EVENTS		( \
+	(1 << DM_EVENT_MOUNT)		| \
+	(1 << DM_EVENT_PREUNMOUNT)	| \
+	(1 << DM_EVENT_UNMOUNT)		| \
+	(1 << DM_EVENT_NOSPACE)		| \
+	(1 << DM_EVENT_CREATE)		| \
+	(1 << DM_EVENT_POSTCREATE)	| \
+	(1 << DM_EVENT_REMOVE)		| \
+	(1 << DM_EVENT_POSTREMOVE)	| \
+	(1 << DM_EVENT_RENAME)		| \
+	(1 << DM_EVENT_POSTRENAME)	| \
+	(1 << DM_EVENT_LINK)		| \
+	(1 << DM_EVENT_POSTLINK)	| \
+	(1 << DM_EVENT_SYMLINK)		| \
+	(1 << DM_EVENT_POSTSYMLINK)	| \
+	(1 << DM_EVENT_READ)		| \
+	(1 << DM_EVENT_WRITE)		| \
+	(1 << DM_EVENT_TRUNCATE)	| \
+	(1 << DM_EVENT_ATTRIBUTE)	| \
+	(1 << DM_EVENT_DESTROY)		)
+
+
+/*
+ *	Definitions used for the flags field on dm_send_*_event().
+ */
+
+#define DM_FLAGS_NDELAY		0x001	/* return -EAGAIN after dm_pending() */
+#define DM_FLAGS_UNWANTED	0x002	/* event not in fsys dm_eventset_t */
+#define DM_FLAGS_IMUX		0x004	/* thread holds i_mutex */
+#define DM_FLAGS_IALLOCSEM_RD	0x010	/* thread holds i_alloc_sem rd */
+#define DM_FLAGS_IALLOCSEM_WR	0x020	/* thread holds i_alloc_sem wr */
+
+/*
+ *	Pull in platform specific event flags defines
+ */
+#include "xfs_dmapi_priv.h"
+
+/*
+ *	Macros to turn caller specified delay/block flags into
+ *	dm_send_xxxx_event flag DM_FLAGS_NDELAY.
+ */
+
+#define FILP_DELAY_FLAG(filp) ((filp->f_flags&(O_NDELAY|O_NONBLOCK)) ? \
+			DM_FLAGS_NDELAY : 0)
+
+/*
+ * Prototypes and functions for the Data Migration subsystem.
+ */
+
+typedef int	(*xfs_send_data_t)(int, struct xfs_inode *,
+			loff_t, size_t, int, int *);
+typedef int	(*xfs_send_mmap_t)(struct vm_area_struct *, uint);
+typedef int	(*xfs_send_destroy_t)(struct xfs_inode *, dm_right_t);
+typedef int	(*xfs_send_namesp_t)(dm_eventtype_t, struct xfs_mount *,
+			struct xfs_inode *, dm_right_t,
+			struct xfs_inode *, dm_right_t,
+			const unsigned char *, const unsigned char *,
+			mode_t, int, int);
+typedef int	(*xfs_send_mount_t)(struct xfs_mount *, dm_right_t,
+			char *, char *);
+typedef void	(*xfs_send_unmount_t)(struct xfs_mount *, struct xfs_inode *,
+			dm_right_t, mode_t, int, int);
+
+typedef struct xfs_dmops {
+	xfs_send_data_t		xfs_send_data;
+	xfs_send_mmap_t		xfs_send_mmap;
+	xfs_send_destroy_t	xfs_send_destroy;
+	xfs_send_namesp_t	xfs_send_namesp;
+	xfs_send_mount_t	xfs_send_mount;
+	xfs_send_unmount_t	xfs_send_unmount;
+} xfs_dmops_t;
+
+#define XFS_DMAPI_UNMOUNT_FLAGS(mp) \
+	(((mp)->m_dmevmask & (1 << DM_EVENT_UNMOUNT)) ? 0 : DM_FLAGS_UNWANTED)
+
+#define XFS_DMAPI_PREUNMOUNT_FLAGS(mp) \
+	(((mp)->m_dmevmask & (1 << DM_EVENT_PREUNMOUNT)) \
+		? 0 : DM_FLAGS_UNWANTED)
+
+#define XFS_SEND_DATA(mp, ev,ip,off,len,fl,lock) \
+	(*(mp)->m_dm_ops->xfs_send_data)(ev,ip,off,len,fl,lock)
+#define XFS_SEND_MMAP(mp, vma,fl) \
+	(*(mp)->m_dm_ops->xfs_send_mmap)(vma,fl)
+#define XFS_SEND_DESTROY(mp, ip,right) \
+	(*(mp)->m_dm_ops->xfs_send_destroy)(ip,right)
+#define XFS_SEND_NAMESP(mp, ev,b1,r1,b2,r2,n1,n2,mode,rval,fl) \
+	(*(mp)->m_dm_ops->xfs_send_namesp)(ev,NULL,b1,r1,b2,r2,n1,n2,mode,rval,fl)
+#define XFS_SEND_MOUNT(mp,right,path,name) \
+	(*(mp)->m_dm_ops->xfs_send_mount)(mp,right,path,name)
+#define XFS_SEND_PREUNMOUNT(mp) \
+do { \
+	if (mp->m_flags & XFS_MOUNT_DMAPI) { \
+		(*(mp)->m_dm_ops->xfs_send_namesp)(DM_EVENT_PREUNMOUNT, mp, \
+			(mp)->m_rootip, DM_RIGHT_NULL, \
+			(mp)->m_rootip, DM_RIGHT_NULL, \
+			NULL, NULL, 0, 0, XFS_DMAPI_PREUNMOUNT_FLAGS(mp)); \
+	} \
+} while (0)
+#define XFS_SEND_UNMOUNT(mp) \
+do { \
+	if (mp->m_flags & XFS_MOUNT_DMAPI) { \
+		(*(mp)->m_dm_ops->xfs_send_unmount)(mp, (mp)->m_rootip, \
+			DM_RIGHT_NULL, 0, 0, XFS_DMAPI_UNMOUNT_FLAGS(mp)); \
+	} \
+} while (0)
+
+
+
+
+
+#endif  /* __XFS_DMAPI_H__ */
--- /dev/null
+++ b/fs/xfs/xfs_dmapi_priv.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2000-2006 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef __XFS_DMAPI_PRIV_H__
+#define __XFS_DMAPI_PRIV_H__
+
+/*
+ *	Based on IO_ISDIRECT, decide which i_ flag is set.
+ */
+#define DM_SEM_FLAG_RD(ioflags) (((ioflags) & IO_ISDIRECT) ? \
+			      DM_FLAGS_IMUX : 0)
+#define DM_SEM_FLAG_WR	(DM_FLAGS_IALLOCSEM_WR | DM_FLAGS_IMUX)
+
+#endif /*__XFS_DMAPI_PRIV_H__*/
--- /dev/null
+++ b/fs/xfs/xfs_dmops.c
@@ -0,0 +1,72 @@
+/*
+ * Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#include "xfs.h"
+#include "xfs_fs.h"
+#include "xfs_types.h"
+#include "xfs_log_format.h"
+#include "xfs_log.h"
+#include "xfs_trans.h"
+#include "xfs_trans_resv.h"
+#include "xfs_format.h"
+#include "xfs_sb.h"
+#include "xfs_dmapi.h"
+#include "xfs_mount.h"
+
+int  fs_noerr(void) { return 0; }
+int  fs_nosys(void) { return -ENOSYS; }
+void fs_noval(void) { return; }
+
+static struct xfs_dmops xfs_dmcore_stub = {
+	.xfs_send_data		= (xfs_send_data_t)fs_nosys,
+	.xfs_send_mmap		= (xfs_send_mmap_t)fs_noerr,
+	.xfs_send_destroy	= (xfs_send_destroy_t)fs_nosys,
+	.xfs_send_namesp	= (xfs_send_namesp_t)fs_nosys,
+	.xfs_send_mount		= (xfs_send_mount_t)fs_nosys,
+	.xfs_send_unmount	= (xfs_send_unmount_t)fs_noerr,
+};
+
+int
+xfs_dmops_get(struct xfs_mount *mp)
+{
+	if (mp->m_flags & XFS_MOUNT_DMAPI) {
+		struct xfs_dmops *ops;
+
+		ops = symbol_get(xfs_dmcore_xfs);
+		if (!ops) {
+			request_module("xfs_dmapi");
+			ops = symbol_get(xfs_dmcore_xfs);
+		}
+
+		if (!ops) {
+			xfs_warn(mp, "no dmapi support available.");
+			return -EINVAL;
+		}
+		mp->m_dm_ops = ops;
+	} else {
+		mp->m_dm_ops = &xfs_dmcore_stub;
+	}
+
+	return 0;
+}
+
+void
+xfs_dmops_put(struct xfs_mount *mp)
+{
+	if (mp->m_dm_ops != &xfs_dmcore_stub)
+		symbol_put(xfs_dmcore_xfs);
+}
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@ -37,6 +37,7 @@
 #include "xfs_log.h"
 #include "xfs_icache.h"
 #include "xfs_pnfs.h"
+#include "xfs_dmapi.h"
 
 #include <linux/dcache.h>
 #include <linux/falloc.h>
@@ -44,6 +45,9 @@
 #include <linux/backing-dev.h>
 
 static const struct vm_operations_struct xfs_file_vm_ops;
+#ifdef HAVE_DMAPI
+static struct vm_operations_struct xfs_dmapi_file_vm_ops;
+#endif
 
 /*
  * Locking primitives for read and write IO paths to ensure we consistently use
@@ -203,13 +207,12 @@ xfs_dir_fsync(
 }
 
 STATIC int
-xfs_file_fsync(
-	struct file		*file,
+__xfs_fsync(
+	struct inode		*inode,
 	loff_t			start,
 	loff_t			end,
 	int			datasync)
 {
-	struct inode		*inode = file->f_mapping->host;
 	struct xfs_inode	*ip = XFS_I(inode);
 	struct xfs_mount	*mp = ip->i_mount;
 	int			error = 0;
@@ -283,6 +286,22 @@ xfs_file_fsync(
 	return error;
 }
 
+STATIC int
+xfs_file_fsync(
+	struct file		*file,
+	loff_t			start,
+	loff_t			end,
+	int			datasync)
+{
+	return __xfs_fsync(file->f_mapping->host, start, end, datasync);
+}
+
+int
+xfs_fsync(struct xfs_inode *ip, int datasync)
+{
+	return __xfs_fsync(VFS_I(ip), 0, LLONG_MAX, datasync);
+}
+
 STATIC ssize_t
 xfs_file_read_iter(
 	struct kiocb		*iocb,
@@ -338,6 +357,24 @@ xfs_file_read_iter(
 	 * serialisation.
 	 */
 	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
+
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_READ) && !(ioflags & XFS_IO_INVIS)) {
+		int dmflags = FILP_DELAY_FLAG(file);
+		int iolock = XFS_IOLOCK_SHARED;
+
+		/*
+		 * no need to set DM_FLAGS_IMUX. xfs_rw_ilock() will not
+		 * enable the inode mutex for a XFS_IOLOCK_SHARED lock type
+		 */
+
+		ret = XFS_SEND_DATA(mp, DM_EVENT_READ, ip, iocb->ki_pos, size,
+				     dmflags, &iolock);
+		if (ret) {
+			xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
+			return ret;
+		}
+	}
+
 	if ((ioflags & XFS_IO_ISDIRECT) && inode->i_mapping->nrpages) {
 		xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
 		xfs_rw_ilock(ip, XFS_IOLOCK_EXCL);
@@ -404,6 +441,19 @@ xfs_file_splice_read(
 
 	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
 
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_READ) && !(ioflags & XFS_IO_INVIS)) {
+		struct xfs_mount	*mp = ip->i_mount;
+		int iolock = XFS_IOLOCK_SHARED;
+		int error;
+
+		error = XFS_SEND_DATA(mp, DM_EVENT_READ, ip, *ppos, count,
+					FILP_DELAY_FLAG(infilp), &iolock);
+		if (error) {
+			xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+			return error;
+		}
+	}
+
 	trace_xfs_file_splice_read(ip, count, *ppos, ioflags);
 
 	/* for dax, we need to avoid the page cache */
@@ -580,7 +630,8 @@ STATIC ssize_t
 xfs_file_aio_write_checks(
 	struct kiocb		*iocb,
 	struct iov_iter		*from,
-	int			*iolock)
+	int			*iolock,
+	int			*eventsent)
 {
 	struct file		*file = iocb->ki_filp;
 	struct inode		*inode = file->f_mapping->host;
@@ -594,6 +645,33 @@ restart:
 	if (error <= 0)
 		return error;
 
+	if ((DM_EVENT_ENABLED(ip, DM_EVENT_WRITE) &&
+	    !(file->f_mode & FMODE_NOCMTIME) && !*eventsent)) {
+		int	dmflags = FILP_DELAY_FLAG(file);
+
+		if (*iolock & XFS_IOLOCK_EXCL)
+			 dmflags |= DM_FLAGS_IMUX; /* dmapi disable mutex flg */
+
+		error = XFS_SEND_DATA(ip->i_mount, DM_EVENT_WRITE, ip,
+				       iocb->ki_pos, count, dmflags, iolock);
+		if (error)
+			return error;
+
+	/* DMAPI NOSPACE will call this routine again. eventsent is retained
+	 * to ensure that DM event is only sent once.
+	 */
+		*eventsent = 1;
+		/*
+		 * The iolock was dropped and reacquired in XFS_SEND_DATA
+		 * so we have to recheck the size when appending.
+		 * We will only "goto start;" once, since having sent the
+		 * event prevents another call to XFS_SEND_DATA, which is
+		 * what allows the size to change in the first place.
+		 */
+		if ((file->f_flags & O_APPEND) && iocb->ki_pos != XFS_ISIZE(ip))
+			goto restart;
+	}
+
 	error = xfs_break_layouts(inode, iolock, true);
 	if (error)
 		return error;
@@ -700,7 +778,8 @@ restart:
 STATIC ssize_t
 xfs_file_dio_aio_write(
 	struct kiocb		*iocb,
-	struct iov_iter		*from)
+	struct iov_iter		*from,
+	int			*eventsent)
 {
 	struct file		*file = iocb->ki_filp;
 	struct address_space	*mapping = file->f_mapping;
@@ -749,7 +828,7 @@ xfs_file_dio_aio_write(
 		xfs_rw_ilock(ip, iolock);
 	}
 
-	ret = xfs_file_aio_write_checks(iocb, from, &iolock);
+	ret = xfs_file_aio_write_checks(iocb, from, &iolock, eventsent);
 	if (ret)
 		goto out;
 	count = iov_iter_count(from);
@@ -815,7 +894,8 @@ out:
 STATIC ssize_t
 xfs_file_buffered_aio_write(
 	struct kiocb		*iocb,
-	struct iov_iter		*from)
+	struct iov_iter		*from,
+	int			*eventsent)
 {
 	struct file		*file = iocb->ki_filp;
 	struct address_space	*mapping = file->f_mapping;
@@ -827,7 +907,7 @@ xfs_file_buffered_aio_write(
 
 	xfs_rw_ilock(ip, iolock);
 
-	ret = xfs_file_aio_write_checks(iocb, from, &iolock);
+	ret = xfs_file_aio_write_checks(iocb, from, &iolock, eventsent);
 	if (ret)
 		goto out;
 
@@ -882,6 +962,7 @@ xfs_file_write_iter(
 	struct xfs_inode	*ip = XFS_I(inode);
 	ssize_t			ret;
 	size_t			ocount = iov_iter_count(from);
+	int			eventsent = 0;
 
 	XFS_STATS_INC(ip->i_mount, xs_write_calls);
 
@@ -891,10 +972,24 @@ xfs_file_write_iter(
 	if (XFS_FORCED_SHUTDOWN(ip->i_mount))
 		return -EIO;
 
+start:
 	if ((iocb->ki_flags & IOCB_DIRECT) || IS_DAX(inode))
-		ret = xfs_file_dio_aio_write(iocb, from);
+		ret = xfs_file_dio_aio_write(iocb, from, &eventsent);
 	else
-		ret = xfs_file_buffered_aio_write(iocb, from);
+ 		ret = xfs_file_buffered_aio_write(iocb, from, &eventsent);
+
+	if (ret == -ENOSPC &&
+	    DM_EVENT_ENABLED(ip, DM_EVENT_NOSPACE) &&
+	    !(file->f_mode & FMODE_NOCMTIME)) {
+		ret = XFS_SEND_NAMESP(ip->i_mount, DM_EVENT_NOSPACE, ip,
+				DM_RIGHT_NULL, ip, DM_RIGHT_NULL, NULL, NULL,
+				 0, 0, 0); /* Delay flag intentionally unused */
+		if (!ret)
+			goto start;
+		/* error will goto out_unlock below */
+	}
+	if (ret <= 0)
+		return ret;
 
 	if (ret > 0) {
 		ssize_t err;
@@ -1086,6 +1181,23 @@ xfs_file_release(
 	return xfs_release(XFS_I(inode));
 }
 
+#ifdef HAVE_DMAPI
+STATIC int
+xfs_vm_fault(
+	struct vm_area_struct	*vma,
+	struct vm_fault	*vmf)
+{
+	struct inode	*inode = vma->vm_file->f_path.dentry->d_inode;
+	struct xfs_mount *mp = XFS_M(inode->i_sb);
+
+	ASSERT_ALWAYS(mp->m_flags & XFS_MOUNT_DMAPI);
+
+	if (XFS_SEND_MMAP(mp, vma, 0))
+		return VM_FAULT_SIGBUS;
+	return filemap_fault(vma, vmf);
+}
+#endif /* HAVE_DMAPI */
+
 STATIC int
 xfs_file_readdir(
 	struct file	*file,
@@ -1510,11 +1622,21 @@ xfs_filemap_page_mkwrite(
 	struct vm_fault		*vmf)
 {
 	struct inode		*inode = file_inode(vma->vm_file);
-	int			ret;
+	int			ret = 0;
 
 	trace_xfs_filemap_page_mkwrite(XFS_I(inode));
 
 	sb_start_pagefault(inode->i_sb);
+
+#ifdef HAVE_DMAPI
+	if (XFS_M(inode->i_sb)->m_flags & XFS_MOUNT_DMAPI) {
+		if (vma->vm_flags & VM_MAYSHARE) {
+			ret = XFS_SEND_MMAP(XFS_M(inode->i_sb), vma, VM_WRITE);
+			if (ret)
+				return block_page_mkwrite_return(ret);
+		}
+	}
+#endif /* HAVE_DMAPI */
 	file_update_time(vma->vm_file);
 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 
@@ -1649,11 +1771,38 @@ xfs_file_mmap(
 {
 	file_accessed(filp);
 	vma->vm_ops = &xfs_file_vm_ops;
+
+#ifdef HAVE_DMAPI
+	if (XFS_M(filp->f_path.dentry->d_inode->i_sb)->m_flags & XFS_MOUNT_DMAPI)
+		vma->vm_ops = &xfs_dmapi_file_vm_ops;
+#endif /* HAVE_DMAPI */
+
 	if (IS_DAX(file_inode(filp)))
 		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 	return 0;
 }
 
+
+#ifdef HAVE_FOP_OPEN_EXEC
+/* If the user is attempting to execute a file that is offline then
+ * we have to trigger a DMAPI READ event before the file is marked as busy
+ * otherwise the invisible I/O will not be able to write to the file to bring
+ * it back online.
+ */
+STATIC int
+xfs_file_open_exec(
+	struct inode	*inode)
+{
+	struct xfs_mount *mp = XFS_M(inode->i_sb);
+	struct xfs_inode *ip = XFS_I(inode);
+
+	if (unlikely(mp->m_flags & XFS_MOUNT_DMAPI) &&
+	             DM_EVENT_ENABLED(ip, DM_EVENT_READ))
+		return XFS_SEND_DATA(mp, DM_EVENT_READ, ip, 0, 0, 0, NULL);
+	return 0;
+}
+#endif /* HAVE_FOP_OPEN_EXEC */
+
 const struct file_operations xfs_file_operations = {
 	.llseek		= xfs_file_llseek,
 	.read_iter	= xfs_file_read_iter,
@@ -1669,6 +1818,9 @@ const struct file_operations xfs_file_op
 	.release	= xfs_file_release,
 	.fsync		= xfs_file_fsync,
 	.fallocate	= xfs_file_fallocate,
+#ifdef HAVE_FOP_OPEN_EXEC
+	.open_exec	= xfs_file_open_exec,
+#endif
 };
 
 const struct file_operations xfs_dir_file_operations = {
@@ -1682,3 +1834,10 @@ const struct file_operations xfs_dir_fil
 #endif
 	.fsync		= xfs_dir_fsync,
 };
+
+#ifdef HAVE_DMAPI
+static struct vm_operations_struct xfs_dmapi_file_vm_ops = {
+	.fault		= xfs_vm_fault,
+	.page_mkwrite	= xfs_filemap_page_mkwrite,
+};
+#endif /* HAVE_DMAPI */
--- a/fs/xfs/xfs_inode.c
+++ b/fs/xfs/xfs_inode.c
@@ -48,6 +48,7 @@
 #include "xfs_trans_priv.h"
 #include "xfs_log.h"
 #include "xfs_bmap_btree.h"
+#include "xfs_dmapi.h"
 
 kmem_zone_t *xfs_inode_zone;
 
@@ -1156,6 +1157,16 @@ xfs_create(
 	if (XFS_FORCED_SHUTDOWN(mp))
 		return -EIO;
 
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_CREATE)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_CREATE,
+				dp, DM_RIGHT_NULL, NULL,
+				DM_RIGHT_NULL, name->name, NULL,
+				mode, 0, 0);
+
+		if (error)
+			return error;
+	}
+
 	prid = xfs_get_initial_prid(dp);
 
 	/*
@@ -1288,7 +1299,16 @@ xfs_create(
 	xfs_qm_dqrele(pdqp);
 
 	*ipp = ip;
-	return 0;
+
+	/* Fallthrough to std_return with error = 0  */
+ std_return:
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_POSTCREATE)) {
+		XFS_SEND_NAMESP(mp, DM_EVENT_POSTCREATE, dp, DM_RIGHT_NULL,
+				ip, DM_RIGHT_NULL, name->name, NULL, mode,
+				error, 0);
+	}
+
+	return error;
 
  out_bmap_cancel:
 	xfs_bmap_cancel(&free_list);
@@ -1311,7 +1331,8 @@ xfs_create(
 
 	if (unlock_dp_on_error)
 		xfs_iunlock(dp, XFS_IOLOCK_EXCL | XFS_ILOCK_EXCL);
-	return error;
+
+	goto std_return;
 }
 
 int
@@ -1437,6 +1458,17 @@ xfs_link(
 	if (XFS_FORCED_SHUTDOWN(mp))
 		return -EIO;
 
+	if (DM_EVENT_ENABLED(tdp, DM_EVENT_LINK)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_LINK,
+					tdp, DM_RIGHT_NULL,
+					sip, DM_RIGHT_NULL,
+					target_name->name, NULL, 0, 0, 0);
+		if (error)
+			return error;
+	}
+
+	/* Return through std_return after this point. */
+
 	error = xfs_qm_dqattach(sip, 0);
 	if (error)
 		goto std_return;
@@ -1512,12 +1544,24 @@ xfs_link(
 		goto error_return;
 	}
 
-	return xfs_trans_commit(tp);
+	error = xfs_trans_commit(tp);
+	if (error)
+		goto std_return;
 
- error_return:
-	xfs_trans_cancel(tp);
+	/* Fall through to std_return with error = 0. */
  std_return:
+	if (DM_EVENT_ENABLED(sip, DM_EVENT_POSTLINK)) {
+		(void) XFS_SEND_NAMESP(mp, DM_EVENT_POSTLINK,
+				tdp, DM_RIGHT_NULL,
+				sip, DM_RIGHT_NULL,
+				target_name->name, NULL, 0, error, 0);
+	}
 	return error;
+
+	/* FALLTHROUGH */
+ error_return:
+	xfs_trans_cancel(tp);
+	goto std_return;
 }
 
 /*
@@ -1884,6 +1928,9 @@ xfs_inactive(
 
 	mp = ip->i_mount;
 
+	if (ip->i_d.di_nlink == 0 && DM_EVENT_ENABLED(ip, DM_EVENT_DESTROY))
+		XFS_SEND_DESTROY(mp, ip, DM_RIGHT_NULL);
+
 	/* If this is a read-only mount, don't do this (would generate I/O) */
 	if (mp->m_flags & XFS_MOUNT_RDONLY)
 		return;
@@ -2531,6 +2578,14 @@ xfs_remove(
 	if (XFS_FORCED_SHUTDOWN(mp))
 		return -EIO;
 
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_REMOVE)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_REMOVE, dp, DM_RIGHT_NULL,
+					NULL, DM_RIGHT_NULL, name->name, NULL,
+					ip->i_d.di_mode, 0, 0);
+		if (error)
+			return error;
+	}
+
 	error = xfs_qm_dqattach(dp, 0);
 	if (error)
 		goto std_return;
@@ -2635,14 +2690,20 @@ xfs_remove(
 	if (is_dir && xfs_inode_is_filestream(ip))
 		xfs_filestream_deassociate(ip);
 
-	return 0;
+std_return:
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_POSTREMOVE)) {
+		XFS_SEND_NAMESP(mp, DM_EVENT_POSTREMOVE, dp, DM_RIGHT_NULL,
+				NULL, DM_RIGHT_NULL, name->name, NULL,
+				ip->i_d.di_mode, error, 0);
+	}
+
+	return error;
 
  out_bmap_cancel:
 	xfs_bmap_cancel(&free_list);
  out_trans_cancel:
 	xfs_trans_cancel(tp);
- std_return:
-	return error;
+	goto std_return;
 }
 
 /*
@@ -2904,6 +2965,18 @@ xfs_rename(
 
 	trace_xfs_rename(src_dp, target_dp, src_name, target_name);
 
+	if (DM_EVENT_ENABLED(src_dp, DM_EVENT_RENAME) ||
+	    DM_EVENT_ENABLED(target_dp, DM_EVENT_RENAME)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_RENAME,
+					src_dp, DM_RIGHT_NULL,
+					target_dp, DM_RIGHT_NULL,
+					src_name->name, target_name->name,
+					0, 0, 0);
+		if (error)
+			return error;
+	}
+	/* Return through std_return after this point. */
+
 	if ((flags & RENAME_EXCHANGE) && !target_ip)
 		return -EINVAL;
 
@@ -3164,6 +3237,18 @@ xfs_rename(
 	error = xfs_finish_rename(tp, &free_list);
 	if (wip)
 		IRELE(wip);
+
+	/* Fall through to std_return with error = 0 or errno from
+	 * xfs_trans_commit	 */
+ std_return:
+	if (DM_EVENT_ENABLED(src_dp, DM_EVENT_POSTRENAME) ||
+	    DM_EVENT_ENABLED(target_dp, DM_EVENT_POSTRENAME)) {
+		(void) XFS_SEND_NAMESP (mp, DM_EVENT_POSTRENAME,
+					src_dp, DM_RIGHT_NULL,
+					target_dp, DM_RIGHT_NULL,
+					src_name->name, target_name->name,
+					0, error, 0);
+	}
 	return error;
 
 out_bmap_cancel:
@@ -3172,7 +3257,7 @@ out_trans_cancel:
 	xfs_trans_cancel(tp);
 	if (wip)
 		IRELE(wip);
-	return error;
+	goto std_return;
 }
 
 STATIC int
--- a/fs/xfs/xfs_inode.h
+++ b/fs/xfs/xfs_inode.h
@@ -387,6 +387,7 @@ int		xfs_rename(struct xfs_inode *src_dp
 			   struct xfs_inode *src_ip, struct xfs_inode *target_dp,
 			   struct xfs_name *target_name,
 			   struct xfs_inode *target_ip, unsigned int flags);
+int		xfs_fsync(struct xfs_inode *ip, int datasync);
 
 void		xfs_ilock(xfs_inode_t *, uint);
 int		xfs_ilock_nowait(xfs_inode_t *, uint);
--- a/fs/xfs/xfs_ioctl.c
+++ b/fs/xfs/xfs_ioctl.c
@@ -41,6 +41,7 @@
 #include "xfs_trans.h"
 #include "xfs_pnfs.h"
 #include "xfs_acl.h"
+#include "xfs_dmapi.h"
 
 #include <linux/capability.h>
 #include <linux/dcache.h>
@@ -622,6 +623,7 @@ xfs_ioc_space(
 	enum xfs_prealloc_flags	flags = 0;
 	uint			iolock = XFS_IOLOCK_EXCL;
 	int			error;
+	bool			dmapi_nonblock;
 
 	/*
 	 * Only allow the sys admin to reserve space unless
@@ -640,6 +642,9 @@ xfs_ioc_space(
 	if (!S_ISREG(inode->i_mode))
 		return -EINVAL;
 
+	if (filp->f_flags & (O_NDELAY|O_NONBLOCK))
+		dmapi_nonblock = true;
+
 	if (filp->f_flags & O_DSYNC)
 		flags |= XFS_PREALLOC_SYNC;
 	if (ioflags & XFS_IO_INVIS)
@@ -714,7 +719,8 @@ xfs_ioc_space(
 		break;
 	case XFS_IOC_UNRESVSP:
 	case XFS_IOC_UNRESVSP64:
-		error = xfs_free_file_space(ip, bf->l_start, bf->l_len);
+		error = __xfs_free_file_space(ip, bf->l_start, bf->l_len,
+					    dmapi_nonblock, false);
 		break;
 	case XFS_IOC_ALLOCSP:
 	case XFS_IOC_ALLOCSP64:
@@ -787,11 +793,11 @@ xfs_ioc_bulkstat(
 					bulkreq.ubuffer, xfs_inumbers_fmt);
 	else if (cmd == XFS_IOC_FSBULKSTAT_SINGLE)
 		error = xfs_bulkstat_one(mp, inlast, bulkreq.ubuffer,
-					sizeof(xfs_bstat_t), NULL, &done);
+					sizeof(xfs_bstat_t), NULL, NULL, &done);
 	else	/* XFS_IOC_FSBULKSTAT */
 		error = xfs_bulkstat(mp, &inlast, &count, xfs_bulkstat_one,
-				     sizeof(xfs_bstat_t), bulkreq.ubuffer,
-				     &done);
+				     NULL, sizeof(xfs_bstat_t),
+				     bulkreq.ubuffer, &done);
 
 	if (error)
 		return error;
@@ -1178,7 +1184,8 @@ xfs_ioctl_setattr_check_projid(
 STATIC int
 xfs_ioctl_setattr(
 	xfs_inode_t		*ip,
-	struct fsxattr		*fa)
+	struct fsxattr		*fa,
+	bool			dmapi_nonblock)
 {
 	struct xfs_mount	*mp = ip->i_mount;
 	struct xfs_trans	*tp;
@@ -1273,7 +1280,16 @@ xfs_ioctl_setattr(
 	xfs_qm_dqrele(udqp);
 	xfs_qm_dqrele(pdqp);
 
-	return code;
+	if (code)
+		return code;
+
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_ATTRIBUTE)) {
+		XFS_SEND_NAMESP(mp, DM_EVENT_ATTRIBUTE, ip, DM_RIGHT_NULL,
+				NULL, DM_RIGHT_NULL, NULL, NULL, 0, 0,
+				dmapi_nonblock ? DM_FLAGS_NDELAY : 0);
+	}
+
+	return 0;
 
 error_trans_cancel:
 	xfs_trans_cancel(tp);
@@ -1291,14 +1307,18 @@ xfs_ioc_fssetxattr(
 {
 	struct fsxattr		fa;
 	int error;
+	bool			dmapi_nonblock = false;
 
 	if (copy_from_user(&fa, arg, sizeof(fa)))
 		return -EFAULT;
 
+	if (filp->f_flags & (O_NDELAY|O_NONBLOCK))
+		dmapi_nonblock = true;
+
 	error = mnt_want_write_file(filp);
 	if (error)
 		return error;
-	error = xfs_ioctl_setattr(ip, &fa);
+	error = xfs_ioctl_setattr(ip, &fa, dmapi_nonblock);
 	mnt_drop_write_file(filp);
 	return error;
 }
--- a/fs/xfs/xfs_ioctl32.c
+++ b/fs/xfs/xfs_ioctl32.c
@@ -229,6 +229,7 @@ xfs_bulkstat_one_compat(
 	xfs_ino_t	ino,		/* inode number to get data for */
 	void		__user *buffer,	/* buffer to place output in */
 	int		ubsize,		/* size of buffer */
+	void		*private_data,	/* my private data */
 	int		*ubused,	/* bytes used by me */
 	int		*stat)		/* BULKSTAT_RV_... */
 {
@@ -287,11 +288,11 @@ xfs_compat_ioc_bulkstat(
 		int res;
 
 		error = xfs_bulkstat_one_compat(mp, inlast, bulkreq.ubuffer,
-				sizeof(compat_xfs_bstat_t), NULL, &res);
+				sizeof(compat_xfs_bstat_t), NULL, 0, &res);
 	} else if (cmd == XFS_IOC_FSBULKSTAT_32) {
 		error = xfs_bulkstat(mp, &inlast, &count,
-			xfs_bulkstat_one_compat, sizeof(compat_xfs_bstat_t),
-			bulkreq.ubuffer, &done);
+			xfs_bulkstat_one_compat, NULL,
+			sizeof(compat_xfs_bstat_t), bulkreq.ubuffer, &done);
 	} else
 		error = -EINVAL;
 	if (error)
--- a/fs/xfs/xfs_iops.c
+++ b/fs/xfs/xfs_iops.c
@@ -38,6 +38,8 @@
 #include "xfs_dir2.h"
 #include "xfs_trans_space.h"
 #include "xfs_pnfs.h"
+#include "xfs_dmapi.h"
+#include "dmapi/xfs_dm.h"
 
 #include <linux/capability.h>
 #include <linux/xattr.h>
@@ -727,6 +729,17 @@ xfs_setattr_nonsize(
 			return error;
 	}
 
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_ATTRIBUTE) && !xfs_dmapi_marked()) {
+		int ndelay = 0;
+#ifdef ATTR_NO_BLOCK
+		if (iattr->ia_valid & ATTR_NO_BLOCK)
+			ndelay = DM_FLAGS_NDELAY;
+#endif
+		(void) XFS_SEND_NAMESP(mp, DM_EVENT_ATTRIBUTE, ip,
+				       DM_RIGHT_NULL, NULL, DM_RIGHT_NULL,
+				       NULL, NULL, 0, 0, ndelay);
+	}
+
 	return 0;
 
 out_unlock:
@@ -772,6 +785,20 @@ xfs_setattr_size(
 	ASSERT((iattr->ia_valid & (ATTR_UID|ATTR_GID|ATTR_ATIME|ATTR_ATIME_SET|
 		ATTR_MTIME_SET|ATTR_KILL_PRIV|ATTR_TIMES_SET)) == 0);
 
+	if (DM_EVENT_ENABLED(ip, DM_EVENT_TRUNCATE) && !xfs_dmapi_marked()) {
+		int dmflags = DM_SEM_FLAG_WR;
+#ifdef ATTR_NO_BLOCK
+		if (iattr->ia_valid & ATTR_NO_BLOCK)
+			dmflags |= DM_FLAGS_NDELAY;
+#endif
+		error = XFS_SEND_DATA(mp, DM_EVENT_TRUNCATE, ip,
+			iattr->ia_size, 0, dmflags, NULL);
+		if (error) {
+			lock_flags = 0;
+			goto out_unlock;
+		}
+	}
+
 	oldsize = inode->i_size;
 	newsize = iattr->ia_size;
 
@@ -931,6 +958,20 @@ xfs_setattr_size(
 out_unlock:
 	if (lock_flags)
 		xfs_iunlock(ip, lock_flags);
+
+	if (!error &&
+	    DM_EVENT_ENABLED(ip, DM_EVENT_ATTRIBUTE) && !xfs_dmapi_marked()) {
+		int ndelay = 0;
+#ifdef ATTR_NO_BLOCK
+		if (iattr->ia_valid & ATTR_NO_BLOCK)
+			ndelay = DM_FLAGS_NDELAY;
+#endif
+
+		(void) XFS_SEND_NAMESP(mp, DM_EVENT_ATTRIBUTE, ip,
+				       DM_RIGHT_NULL, NULL, DM_RIGHT_NULL,
+				       NULL, NULL, 0, 0, ndelay);
+	}
+
 	return error;
 
 out_trans_cancel:
--- a/fs/xfs/xfs_itable.c
+++ b/fs/xfs/xfs_itable.c
@@ -31,7 +31,7 @@
 #include "xfs_trace.h"
 #include "xfs_icache.h"
 
-STATIC int
+int
 xfs_internal_inum(
 	xfs_mount_t	*mp,
 	xfs_ino_t	ino)
@@ -161,6 +161,7 @@ xfs_bulkstat_one(
 	xfs_ino_t	ino,		/* inode number to get data for */
 	void		__user *buffer,	/* buffer to place output in */
 	int		ubsize,		/* size of buffer */
+	void		*private_data,	/* my private data */
 	int		*ubused,	/* bytes used by me */
 	int		*stat)		/* BULKSTAT_RV_... */
 {
@@ -276,6 +277,7 @@ xfs_bulkstat_ag_ichunk(
 	xfs_agnumber_t			agno,
 	struct xfs_inobt_rec_incore	*irbp,
 	bulkstat_one_pf			formatter,
+	void				*private_data,
 	size_t				statstruct_size,
 	struct xfs_bulkstat_agichunk	*acp,
 	xfs_agino_t			*last_agino)
@@ -301,7 +303,8 @@ xfs_bulkstat_ag_ichunk(
 		/* Get the inode and fill in a single buffer */
 		ubused = statstruct_size;
 		error = formatter(mp, XFS_AGINO_TO_INO(mp, agno, agino),
-				  *ubufp, acp->ac_ubleft, &ubused, &fmterror);
+				  *ubufp, acp->ac_ubleft, private_data,
+ 				  &ubused, &fmterror);
 
 		if (fmterror == BULKSTAT_RV_GIVEUP ||
 		    (error && error != -ENOENT && error != -EINVAL)) {
@@ -343,6 +346,7 @@ xfs_bulkstat(
 	xfs_ino_t		*lastinop, /* last inode returned */
 	int			*ubcountp, /* size of buffer/count returned */
 	bulkstat_one_pf		formatter, /* func that'd fill a single buf */
+	void			*private_data,/* private data for formatter */
 	size_t			statstruct_size, /* sizeof struct filling */
 	char			__user *ubuffer, /* buffer with inode stats */
 	int			*done)	/* 1 if there are more stats to get */
@@ -488,8 +492,8 @@ del_cursor:
 		     irbp < irbufend && ac.ac_ubleft >= statstruct_size;
 		     irbp++) {
 			error = xfs_bulkstat_ag_ichunk(mp, agno, irbp,
-					formatter, statstruct_size, &ac,
-					&agino);
+					formatter, private_data,
+					statstruct_size, &ac, &agino);
 			if (error)
 				break;
 
--- a/fs/xfs/xfs_itable.h
+++ b/fs/xfs/xfs_itable.h
@@ -27,6 +27,7 @@ typedef int (*bulkstat_one_pf)(struct xf
 			       xfs_ino_t	ino,
 			       void		__user *buffer,
 			       int		ubsize,
+			       void		*private_data,
 			       int		*ubused,
 			       int		*stat);
 
@@ -46,6 +47,7 @@ xfs_bulkstat(
 	xfs_ino_t	*lastino,	/* last inode returned */
 	int		*count,		/* size of buffer/count returned */
 	bulkstat_one_pf formatter,	/* func that'd fill a single buf */
+	void		*private_data,	/* private data for formatter */
 	size_t		statstruct_size,/* sizeof struct that we're filling */
 	char		__user *ubuffer,/* buffer with inode stats */
 	int		*done);		/* 1 if there are more stats to get */
@@ -72,9 +74,15 @@ xfs_bulkstat_one(
 	xfs_ino_t		ino,
 	void			__user *buffer,
 	int			ubsize,
+	void			*private_data,
 	int			*ubused,
 	int			*stat);
 
+int
+xfs_internal_inum(
+       xfs_mount_t             *mp,
+       xfs_ino_t               ino);
+
 typedef int (*inumbers_fmt_pf)(
 	void			__user *ubuffer, /* buffer to write to */
 	const xfs_inogrp_t	*buffer,	/* buffer to read from */
--- /dev/null
+++ b/fs/xfs/xfs_ksyms.c
@@ -0,0 +1,91 @@
+/*
+ * Copyright (c) 2004-2008 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+
+#include "xfs.h"
+#include "xfs_fs.h"
+#include "xfs_bit.h"
+#include "xfs_buf.h"
+#include "xfs_log_format.h"
+#include "xfs_log.h"
+#include "xfs_format.h"
+#include "xfs_trans.h"
+#include "xfs_trans_resv.h"
+#include "xfs_sb.h"
+#include "xfs_alloc.h"
+#include "xfs_dmapi.h"
+#include "xfs_format.h"
+#include "xfs_quota.h"
+#include "xfs_mount.h"
+#include "xfs_da_format.h"
+#include "xfs_da_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_dir2.h"
+#include "xfs_dir2_priv.h"
+#include "xfs_attr_sf.h"
+#include "xfs_inode.h"
+#include "xfs_btree.h"
+#include "xfs_ialloc.h"
+#include "xfs_bmap.h"
+#include "xfs_rtalloc.h"
+#include "xfs_error.h"
+#include "xfs_itable.h"
+#include "xfs_acl.h"
+#include "xfs_attr.h"
+#include "xfs_attr_leaf.h"
+#include "xfs_inode_item.h"
+#include "xfs_buf_item.h"
+#include "xfs_extfree_item.h"
+#include "xfs_log_priv.h"
+#include "xfs_trans_priv.h"
+#include "xfs_trans_space.h"
+#include "xfs_iomap.h"
+#include "xfs_filestream.h"
+#include "xfs_icache.h"
+#include "xfs_bmap_util.h"
+#include "xfs_pnfs.h"
+
+EXPORT_SYMBOL(xfs_iunlock);
+EXPORT_SYMBOL(xfs_attr_remove);
+EXPORT_SYMBOL(xfs_iget);
+EXPORT_SYMBOL(xfs_bmapi_read);
+EXPORT_SYMBOL(xfs_internal_inum);
+EXPORT_SYMBOL(xfs_attr_set);
+EXPORT_SYMBOL(xfs_trans_reserve);
+EXPORT_SYMBOL(xfs_trans_ijoin);
+EXPORT_SYMBOL(xfs_free_eofblocks);
+EXPORT_SYMBOL(kmem_free);
+EXPORT_SYMBOL(xfs_trans_commit);
+EXPORT_SYMBOL(xfs_ilock);
+EXPORT_SYMBOL(xfs_attr_get);
+EXPORT_SYMBOL(xfs_readdir);
+EXPORT_SYMBOL(xfs_setattr_size);
+EXPORT_SYMBOL(xfs_setattr_nonsize);
+EXPORT_SYMBOL(xfs_trans_alloc);
+EXPORT_SYMBOL(xfs_trans_cancel);
+EXPORT_SYMBOL(xfs_fsync);
+EXPORT_SYMBOL(xfs_bulkstat);
+EXPORT_SYMBOL(xfs_ilock_data_map_shared);
+EXPORT_SYMBOL(xfs_trans_log_inode);
+EXPORT_SYMBOL(xfs_attr_list);
+EXPORT_SYMBOL(kmem_zalloc_large);
+EXPORT_SYMBOL(__xfs_free_file_space);
+EXPORT_SYMBOL(xfs_get_extsz_hint);
+EXPORT_SYMBOL(assfail);
+EXPORT_SYMBOL(xfs_break_layouts);
--- a/fs/xfs/xfs_linux.h
+++ b/fs/xfs/xfs_linux.h
@@ -210,6 +210,10 @@ static inline kgid_t xfs_gid_to_kgid(__u
 #define xfs_sort(a,n,s,fn)	sort(a,n,s,fn,NULL)
 #define xfs_stack_trace()	dump_stack()
 
+#undef HAVE_DMAPI
+#if defined(CONFIG_XFS_DMAPI) || defined(CONFIG_XFS_DMAPI_MODULE)
+#define HAVE_DMAPI
+#endif
 
 /* Move the kernel do_div definition off to one side */
 
--- a/fs/xfs/xfs_mount.h
+++ b/fs/xfs/xfs_mount.h
@@ -118,6 +118,9 @@ typedef struct xfs_mount {
 	const struct xfs_dir_ops *m_dir_inode_ops; /* vector of dir inode ops */
 	const struct xfs_dir_ops *m_nondir_inode_ops; /* !dir inode ops */
 	uint			m_chsize;	/* size of next field */
+
+	struct xfs_dmops	*m_dm_ops;	/* vector of DMI ops */
+	struct xfs_qmops	*m_qm_ops;	/* vector of XQM ops */
 	atomic_t		m_active_trans;	/* number trans frozen */
 	struct xfs_mru_cache	*m_filestream;  /* per-mount filestream data */
 	struct delayed_work	m_reclaim_work;	/* background inode reclaim */
@@ -147,6 +150,7 @@ typedef struct xfs_mount {
 	 * to various other kinds of pain inflicted on the pNFS server.
 	 */
 	__uint32_t		m_generation;
+	const char		*m_mtpt;
 } xfs_mount_t;
 
 /*
@@ -155,6 +159,7 @@ typedef struct xfs_mount {
 #define XFS_MOUNT_WSYNC		(1ULL << 0)	/* for nfs - all metadata ops
 						   must be synchronous except
 						   for space allocations */
+#define XFS_MOUNT_DMAPI		(1ULL << 2)	/* dmapi is enabled */
 #define XFS_MOUNT_WAS_CLEAN	(1ULL << 3)
 #define XFS_MOUNT_FS_SHUTDOWN	(1ULL << 4)	/* atomic stop of all filesystem
 						   operations, typically for
@@ -338,6 +343,11 @@ extern int	xfs_dev_is_read_only(struct x
 
 extern void	xfs_set_low_space_thresholds(struct xfs_mount *);
 
+extern int	xfs_dmops_get(struct xfs_mount *);
+extern void	xfs_dmops_put(struct xfs_mount *);
+
+extern struct xfs_dmops xfs_dmcore_xfs;
+
 int	xfs_zero_extent(struct xfs_inode *ip, xfs_fsblock_t start_fsb,
 			xfs_off_t count_fsb);
 
--- a/fs/xfs/xfs_qm.c
+++ b/fs/xfs/xfs_qm.c
@@ -1125,6 +1125,7 @@ xfs_qm_dqusage_adjust(
 	xfs_ino_t	ino,		/* inode number to get data for */
 	void		__user *buffer,	/* not used */
 	int		ubsize,		/* not used */
+	void		*private_data,	/* not used */
 	int		*ubused,	/* not used */
 	int		*res)		/* result code value */
 {
@@ -1303,7 +1304,7 @@ xfs_qm_quotacheck(
 		 */
 		error = xfs_bulkstat(mp, &lastino, &count,
 				     xfs_qm_dqusage_adjust,
-				     structsz, NULL, &done);
+				     NULL, structsz, NULL, &done);
 		if (error)
 			break;
 
--- a/fs/xfs/xfs_super.c
+++ b/fs/xfs/xfs_super.c
@@ -45,6 +45,7 @@
 #include "xfs_filestream.h"
 #include "xfs_quota.h"
 #include "xfs_sysfs.h"
+#include "xfs_dmapi.h"
 
 #include <linux/namei.h>
 #include <linux/init.h>
@@ -109,6 +110,9 @@ static struct xfs_kobj xfs_dbg_kobj;	/*
 #define MNTOPT_GQUOTANOENF "gqnoenforce"/* group quota limit enforcement */
 #define MNTOPT_PQUOTANOENF "pqnoenforce"/* project quota limit enforcement */
 #define MNTOPT_QUOTANOENF  "qnoenforce"	/* same as uqnoenforce */
+#define MNTOPT_DMAPI	"dmapi"		/* DMI enabled (DMAPI / XDSM) */
+#define MNTOPT_XDSM	"xdsm"		/* DMI enabled (DMAPI / XDSM) */
+#define MNTOPT_DMI	"dmi"		/* DMI enabled (DMAPI / XDSM) */
 #define MNTOPT_DISCARD	   "discard"	/* Discard unused blocks */
 #define MNTOPT_NODISCARD   "nodiscard"	/* Do not discard unused blocks */
 
@@ -173,13 +177,15 @@ suffix_kstrtoint(char *s, unsigned int b
 STATIC int
 xfs_parseargs(
 	struct xfs_mount	*mp,
-	char			*options)
+	char			*options,
+	char			**mtpt)
 {
 	struct super_block	*sb = mp->m_super;
 	char			*this_char, *value;
 	int			dsunit = 0;
 	int			dswidth = 0;
 	int			iosize = 0;
+	int			dmapi_implies_ikeep = 1;
 	__uint8_t		iosizelog = 0;
 
 	/*
@@ -249,9 +255,14 @@ xfs_parseargs(
 			if (!mp->m_logname)
 				return -ENOMEM;
 		} else if (!strcmp(this_char, MNTOPT_MTPT)) {
-			xfs_warn(mp, "%s option not allowed on this system",
-				this_char);
-			return -EINVAL;
+			if (!value || !*value) {
+				xfs_warn(mp, "%s option requires an argument",
+					 this_char);
+				return -EINVAL;
+			}
+			*mtpt = kstrndup(value, MAXNAMELEN, GFP_KERNEL);
+			if (!*mtpt)
+				return -ENOMEM;
 		} else if (!strcmp(this_char, MNTOPT_RTDEV)) {
 			if (!value || !*value) {
 				xfs_warn(mp, "%s option requires an argument",
@@ -314,6 +325,7 @@ xfs_parseargs(
 		} else if (!strcmp(this_char, MNTOPT_IKEEP)) {
 			mp->m_flags |= XFS_MOUNT_IKEEP;
 		} else if (!strcmp(this_char, MNTOPT_NOIKEEP)) {
+			dmapi_implies_ikeep = 0;
 			mp->m_flags &= ~XFS_MOUNT_IKEEP;
 		} else if (!strcmp(this_char, MNTOPT_LARGEIO)) {
 			mp->m_flags &= ~XFS_MOUNT_COMPAT_IOSIZE;
@@ -353,6 +365,12 @@ xfs_parseargs(
 		} else if (!strcmp(this_char, MNTOPT_GQUOTANOENF)) {
 			mp->m_qflags |= (XFS_GQUOTA_ACCT | XFS_GQUOTA_ACTIVE);
 			mp->m_qflags &= ~XFS_GQUOTA_ENFD;
+		} else if (!strcmp(this_char, MNTOPT_DMAPI)) {
+			mp->m_flags |= XFS_MOUNT_DMAPI;
+		} else if (!strcmp(this_char, MNTOPT_XDSM)) {
+			mp->m_flags |= XFS_MOUNT_DMAPI;
+		} else if (!strcmp(this_char, MNTOPT_DMI)) {
+			mp->m_flags |= XFS_MOUNT_DMAPI;
 		} else if (!strcmp(this_char, MNTOPT_DISCARD)) {
 			mp->m_flags |= XFS_MOUNT_DISCARD;
 		} else if (!strcmp(this_char, MNTOPT_NODISCARD)) {
@@ -389,6 +407,12 @@ xfs_parseargs(
 	}
 #endif
 
+	if ((mp->m_flags & XFS_MOUNT_DMAPI) && (!*mtpt || *mtpt[0] == '\0')) {
+		printk("XFS: %s option needs the mount point option as well\n",
+			MNTOPT_DMAPI);
+		return -EINVAL;
+	}
+
 	if ((dsunit && !dswidth) || (!dsunit && dswidth)) {
 		xfs_warn(mp, "sunit and swidth must be specified together");
 		return -EINVAL;
@@ -401,6 +425,18 @@ xfs_parseargs(
 		return -EINVAL;
 	}
 
+	/*
+	 * Applications using DMI filesystems often expect the
+	 * inode generation number to be monotonically increasing.
+	 * If we delete inode chunks we break this assumption, so
+	 * keep unused inode chunks on disk for DMI filesystems
+	 * until we come up with a better solution.
+	 * Note that if "ikeep" or "noikeep" mount options are
+	 * supplied, then they are honored.
+	 */
+	if ((mp->m_flags & XFS_MOUNT_DMAPI) && dmapi_implies_ikeep)
+		mp->m_flags |= XFS_MOUNT_IKEEP;
+
 done:
 	if (dsunit && !(mp->m_flags & XFS_MOUNT_NOALIGN)) {
 		/*
@@ -469,6 +505,7 @@ xfs_showargs(
 		{ XFS_MOUNT_NORECOVERY,		"," MNTOPT_NORECOVERY },
 		{ XFS_MOUNT_ATTR2,		"," MNTOPT_ATTR2 },
 		{ XFS_MOUNT_FILESTREAMS,	"," MNTOPT_FILESTREAM },
+		{ XFS_MOUNT_DMAPI,		"," MNTOPT_DMAPI },
 		{ XFS_MOUNT_GRPID,		"," MNTOPT_GRPID },
 		{ XFS_MOUNT_DISCARD,		"," MNTOPT_DISCARD },
 		{ XFS_MOUNT_SMALL_INUMS,	"," MNTOPT_32BITINODE },
@@ -1432,6 +1469,7 @@ xfs_fs_fill_super(
 	struct inode		*root;
 	struct xfs_mount	*mp = NULL;
 	int			flags = 0, error = -ENOMEM;
+	char			*mtpt = NULL;
 
 	mp = kzalloc(sizeof(struct xfs_mount), GFP_KERNEL);
 	if (!mp)
@@ -1447,10 +1485,12 @@ xfs_fs_fill_super(
 	mp->m_super = sb;
 	sb->s_fs_info = mp;
 
-	error = xfs_parseargs(mp, (char *)data);
+	error = xfs_parseargs(mp, (char *)data, &mtpt);
 	if (error)
 		goto out_free_fsname;
 
+	mp->m_mtpt = mtpt;
+
 	sb_min_blocksize(sb, BBSIZE);
 	sb->s_xattr = xfs_xattr_handlers;
 	sb->s_export_op = &xfs_export_operations;
@@ -1460,12 +1500,16 @@ xfs_fs_fill_super(
 #endif
 	sb->s_op = &xfs_super_operations;
 
+	error = xfs_dmops_get(mp);
+	if (error)
+		goto out_free_fsname;
+
 	if (silent)
 		flags |= XFS_MFSI_QUIET;
 
 	error = xfs_open_devices(mp);
 	if (error)
-		goto out_free_fsname;
+		goto out_put_dmops;
 
 	error = xfs_init_mount_workqueues(mp);
 	if (error)
@@ -1541,6 +1585,9 @@ xfs_fs_fill_super(
 		error = -ENOENT;
 		goto out_unmount;
 	}
+
+	XFS_SEND_MOUNT(mp, DM_RIGHT_NULL, mtpt, mp->m_fsname);
+
 	sb->s_root = d_make_root(root);
 	if (!sb->s_root) {
 		error = -ENOMEM;
@@ -1561,8 +1608,11 @@ xfs_fs_fill_super(
 	xfs_destroy_mount_workqueues(mp);
  out_close_devices:
 	xfs_close_devices(mp);
+ out_put_dmops:
+	xfs_dmops_put(mp);
  out_free_fsname:
 	xfs_free_fsname(mp);
+	kfree(mtpt);
 	kfree(mp);
  out:
 	return error;
@@ -1580,15 +1630,19 @@ xfs_fs_put_super(
 	struct xfs_mount	*mp = XFS_M(sb);
 
 	xfs_notice(mp, "Unmounting Filesystem");
+	XFS_SEND_PREUNMOUNT(mp);
 	xfs_filestream_unmount(mp);
-	xfs_unmountfs(mp);
+	XFS_SEND_UNMOUNT(mp);
 
+	xfs_unmountfs(mp);
 	xfs_freesb(mp);
 	free_percpu(mp->m_stats.xs_stats);
 	xfs_destroy_percpu_counters(mp);
 	xfs_destroy_mount_workqueues(mp);
 	xfs_close_devices(mp);
+	xfs_dmops_put(mp);
 	xfs_free_fsname(mp);
+	kfree(mp->m_mtpt);
 	kfree(mp);
 }
 
@@ -1634,7 +1688,7 @@ static const struct super_operations xfs
 	.free_cached_objects	= xfs_fs_free_cached_objects,
 };
 
-static struct file_system_type xfs_fs_type = {
+struct file_system_type xfs_fs_type = {
 	.owner			= THIS_MODULE,
 	.name			= "xfs",
 	.mount			= xfs_fs_mount,
@@ -1642,6 +1696,7 @@ static struct file_system_type xfs_fs_ty
 	.fs_flags		= FS_REQUIRES_DEV,
 };
 MODULE_ALIAS_FS("xfs");
+EXPORT_SYMBOL(xfs_fs_type);
 
 STATIC int __init
 xfs_init_zones(void)
--- a/fs/xfs/xfs_super.h
+++ b/fs/xfs/xfs_super.h
@@ -44,6 +44,12 @@ extern void xfs_qm_exit(void);
 # define XFS_REALTIME_STRING
 #endif
 
+#ifdef CONFIG_XFS_DMAPI
+# define XFS_DMAPI_STRING	"dmapi support, "
+#else
+# define XFS_DMAPI_STRING
+#endif
+
 #ifdef DEBUG
 # define XFS_DBG_STRING		"debug"
 #else
@@ -54,6 +60,7 @@ extern void xfs_qm_exit(void);
 #define XFS_BUILD_OPTIONS	XFS_ACL_STRING \
 				XFS_SECURITY_STRING \
 				XFS_REALTIME_STRING \
+				XFS_DMAPI_STRING \
 				XFS_DBG_STRING /* DBG must be last */
 
 struct xfs_inode;
--- a/fs/xfs/xfs_symlink.c
+++ b/fs/xfs/xfs_symlink.c
@@ -40,6 +40,7 @@
 #include "xfs_symlink.h"
 #include "xfs_trans.h"
 #include "xfs_log.h"
+#include "xfs_dmapi.h"
 
 /* ----- Kernel only functions below ----- */
 STATIC int
@@ -208,6 +209,17 @@ xfs_symlink(
 	if (pathlen >= MAXPATHLEN)      /* total string too long */
 		return -ENAMETOOLONG;
 
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_SYMLINK)) {
+		error = XFS_SEND_NAMESP(mp, DM_EVENT_SYMLINK, dp,
+					DM_RIGHT_NULL, NULL, DM_RIGHT_NULL,
+					link_name->name,
+					(unsigned char *)target_path, 0, 0, 0);
+		if (error)
+			return error;
+	}
+
+	/* Return through std_return after this point. */
+
 	udqp = gdqp = NULL;
 	prid = xfs_get_initial_prid(dp);
 
@@ -399,8 +411,21 @@ xfs_symlink(
 	xfs_qm_dqrele(gdqp);
 	xfs_qm_dqrele(pdqp);
 
-	*ipp = ip;
-	return 0;
+	/* Fall through to std_return with error = 0 or errno from
+	 * xfs_trans_commit	*/
+ std_return:
+	if (DM_EVENT_ENABLED(dp, DM_EVENT_POSTSYMLINK)) {
+		(void) XFS_SEND_NAMESP(mp, DM_EVENT_POSTSYMLINK,
+					dp, DM_RIGHT_NULL,
+					error ? NULL : ip,
+					DM_RIGHT_NULL, link_name->name,
+					(unsigned char *)target_path,
+					0, error, 0);
+	}
+
+	if (!error)
+		*ipp = ip;
+	return error;
 
 out_bmap_cancel:
 	xfs_bmap_cancel(&free_list);
@@ -423,7 +448,8 @@ out_release_inode:
 
 	if (unlock_dp_on_error)
 		xfs_iunlock(dp, XFS_IOLOCK_EXCL | XFS_ILOCK_EXCL);
-	return error;
+
+	goto std_return;
 }
 
 /*
--- a/fs/xfs/xfs_trace.c
+++ b/fs/xfs/xfs_trace.c
@@ -52,3 +52,8 @@
  */
 #define CREATE_TRACE_POINTS
 #include "xfs_trace.h"
+
+#ifdef CONFIG_TRACEPOINTS
+/* Needed for DMAPI */
+EXPORT_SYMBOL(__tracepoint_xfs_irele);
+#endif
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -1172,6 +1172,7 @@ DEFINE_RW_EVENT(xfs_file_read);
 DEFINE_RW_EVENT(xfs_file_buffered_write);
 DEFINE_RW_EVENT(xfs_file_direct_write);
 DEFINE_RW_EVENT(xfs_file_splice_read);
+DEFINE_RW_EVENT(xfs_file_splice_write);
 
 DECLARE_EVENT_CLASS(xfs_page_class,
 	TP_PROTO(struct inode *inode, struct page *page, unsigned long off,
